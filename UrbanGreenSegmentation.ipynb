{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from typing import Type\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from pkgs import dataprepare\n",
    "from pkgs import legacytraining\n",
    "from pkgs import neuralnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanGreenSegmentation(pl.LightningModule):\n",
    "    def __init__(self, rotate_training_data : bool = False, train_ratio : float = 0.8, patch_size : int = 100, batch_size : int = 4, region:str = 'N12'):\n",
    "        super(UrbanGreenSegmentation, self).__init__()\n",
    "        raw_data_array, OHE_target_array, raw_target_array = dataprepare.prepare_raw_files(region)\n",
    "        self.batch_size = batch_size\n",
    "        self.Datasets = {\n",
    "            'Train' : dataprepare.TrainDataset3(raw_data_array, OHE_target_array, raw_target_array, patch_size = patch_size, rotate = rotate_training_data, train_ratio = train_ratio),\n",
    "            'Validation' : dataprepare.TrainDataset3(raw_data_array, OHE_target_array, raw_target_array, patch_size = patch_size, is_validating = True, rotate = rotate_training_data, train_ratio = train_ratio),\n",
    "            'Prediction' : dataprepare.TrainDataset3(raw_data_array, OHE_target_array, raw_target_array, patch_size = patch_size, is_evaluating = True, train_ratio = train_ratio)\n",
    "        }\n",
    "\n",
    "        self.Dataloaders = {\n",
    "            'Train' : DataLoader(self.Datasets['Train'], batch_size=batch_size),\n",
    "            'Validation' : DataLoader(self.Datasets['Validation'], batch_size=batch_size),\n",
    "            'Prediction' : DataLoader(self.Datasets['Prediction'], batch_size=batch_size)\n",
    "        }\n",
    "        \n",
    "        # 3개 배치 사용시 메모리 5기가\n",
    "        # 2개 배치 사용시 메모리 3.8기가\n",
    "\n",
    "        self.unet = neuralnet.UNet()\n",
    "        self.regression = neuralnet.Splitted_Regression()\n",
    "        \n",
    "        self.fc1 = nn.Conv2d(in_channels=64, out_channels=7)\n",
    "        self.bn1 = nn.BatchNorm2d(7)\n",
    "        self.bn2 = nn.BatchNorm2d(14)\n",
    "        self.fc2 = nn.Conv2d(in_channels=14, out_channels=7)\n",
    "        self.softmax = nn.Softmax2d()\n",
    "\n",
    "    def forward(self, x_seg, x_reg):\n",
    "        x_reg = self.regression(x_reg)\n",
    "        x_seg = self.unet(x_seg)\n",
    "        x_seg = self.fc1(x_seg)\n",
    "        x_seg = self.bn1(x_seg)\n",
    "        x_seg = torch.cat((x_reg, x_seg), dim=1)\n",
    "        x_seg = self.bn2(x_seg)\n",
    "        x_seg = self.fc2(x_seg)\n",
    "        x_seg = self.softmax(x_seg)\n",
    "        return x_seg\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_seg, x_reg, y_seg, _ = batch\n",
    "        y_hat = self(x_seg, x_reg)\n",
    "        return {'loss' : F.cross_entropy(y_hat, y_seg)}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x_seg, x_reg, y_seg, _ = batch\n",
    "        y_hat = self(x_seg, x_reg)\n",
    "        return {\n",
    "            'valid_loss' : F.cross_entropy(y_hat, y_seg),\n",
    "            'y_hat' : y_hat.detach(),\n",
    "            'y' : y_seg.detach()\n",
    "        }\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        train_optimizer = torch.optim.Adam(self.parameters(), lr=0.02)\n",
    "        train_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(train_optimizer, T_max=10)\n",
    "        return [train_optimizer], [train_scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.Datasets['Train'], batch_size = self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.Datasets['Validation'], batch_size = self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.Datasets['Prediction'], batch_size = self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanGreenSegmentation(nn.Module):\n",
    "    def __init__(self, device = 'cuda:0'):\n",
    "        super(UrbanGreenSegmentation, self).__init__()\n",
    "        \n",
    "        # 3개 배치 사용시 메모리 5기가\n",
    "        # 2개 배치 사용시 메모리 3.8기가\n",
    "\n",
    "        self.unet = neuralnet.UNet()\n",
    "        self.regression = neuralnet.Splitted_Regression(device=device)\n",
    "        \n",
    "        self.fc1 = nn.Conv2d(in_channels=64, out_channels=7, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(7)\n",
    "        self.bn2 = nn.BatchNorm2d(14)\n",
    "        self.fc2 = nn.Conv2d(in_channels=14, out_channels=7, kernel_size=1)\n",
    "        self.softmax = nn.Softmax2d()\n",
    "\n",
    "    def forward(self, x_seg, x_reg):\n",
    "        x_reg = self.regression(x_reg)\n",
    "        x_seg = self.unet(x_seg)\n",
    "        x_seg = self.fc1(x_seg)\n",
    "        x_seg = self.bn1(x_seg)\n",
    "        x_seg = torch.cat((x_reg, x_seg), dim=1)\n",
    "        x_seg = self.bn2(x_seg)\n",
    "        x_seg = self.fc2(x_seg)\n",
    "        x_seg = self.softmax(x_seg)\n",
    "        return x_seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanGreenSegmentation2(nn.Module):\n",
    "    def __init__(self, in_channel:int=6, out_channel:int=7):\n",
    "        super(UrbanGreenSegmentation2, self).__init__()\n",
    "        \n",
    "        # 3개 배치 사용시 메모리 5기가\n",
    "        # 2개 배치 사용시 메모리 3.8기가\n",
    "\n",
    "        self.unet = neuralnet.UNet(in_channel=in_channel)\n",
    "        #self.regression = neuralnet.Splitted_Regression()\n",
    "        \n",
    "        self.fc1 = nn.Conv2d(in_channels=64, out_channels=out_channel, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        #self.bn2 = nn.BatchNorm2d(14)\n",
    "        #self.fc2 = nn.Conv2d(in_channels=14, out_channels=7, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x_seg):\n",
    "        #x_reg = self.regression(x_reg)\n",
    "        x_seg = self.unet(x_seg)\n",
    "        x_seg = self.fc1(x_seg)\n",
    "        x_seg = self.bn1(x_seg)\n",
    "        #x_seg = torch.cat((x_reg, x_seg), dim=1)\n",
    "        #x_seg = self.bn2(x_seg)\n",
    "        #x_seg = self.fc2(x_seg)\n",
    "\n",
    "        return x_seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = 0 # slot number (e.g., 3), no gpu use -> write just ' '\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpus)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 4\n",
    "patch_size = 100\n",
    "train_ratio = 0.8\n",
    "rotate_training_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_array_N12 ,raw_target_array_N12, OHE_target_array_N12 = dataprepare.prepare_raw_files('N12')\n",
    "raw_data_array_H19 ,raw_target_array_H19, OHE_target_array_H19 = dataprepare.prepare_raw_files('H19')\n",
    "raw_data_array_M18 ,raw_target_array_M18, OHE_target_array_M18 = dataprepare.prepare_raw_files('M18')\n",
    "\n",
    "raw_data_array = np.concatenate((raw_data_array_N12, raw_data_array_H19, raw_data_array_M18), axis=-1)\n",
    "print(raw_data_array.shape)\n",
    "raw_target_array = np.concatenate((raw_target_array_N12, raw_target_array_H19, raw_target_array_M18), axis=-1)\n",
    "print(raw_target_array.shape)\n",
    "OHE_target_array = np.concatenate((OHE_target_array_N12, OHE_target_array_H19, OHE_target_array_M18), axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets_ver3 = {\n",
    "    'Train' : dataprepare.TrainDataset4(raw_data_array, OHE_target_array, raw_target_array, patch_size = patch_size, rotate = rotate_training_data, train_ratio = train_ratio),\n",
    "    'Validation' : dataprepare.TrainDataset4(raw_data_array, OHE_target_array, raw_target_array, patch_size = patch_size, is_validating = True, rotate = rotate_training_data, train_ratio = train_ratio),\n",
    "    'Prediction' : dataprepare.TrainDataset4(raw_data_array_H19, OHE_target_array_H19, raw_target_array_H19, patch_size = patch_size, is_evaluating = True, train_ratio = train_ratio)\n",
    "}\n",
    "Dataloaders_ver3 = {\n",
    "    'Train' : DataLoader(Datasets_ver3['Train'], batch_size=batch_size),\n",
    "    'Validation' : DataLoader(Datasets_ver3['Validation'], batch_size=batch_size),\n",
    "    'Prediction' : DataLoader(Datasets_ver3['Prediction'], batch_size=2)\n",
    "}\n",
    "model = UrbanGreenSegmentation()\n",
    "criterion3 = nn.CrossEntropyLoss()\n",
    "optimizer3 = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler3 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer3, 'min', patience=5, factor=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,_,b,_ = Dataloaders_ver3['Validation'].dataset[:]\n",
    "print(a.max())\n",
    "print(b.shape)\n",
    "for i in range(100,120):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(a[i,0,92:192, 92:192])\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(b[i,:,:])\n",
    "#plt.figure(figsize=(10,10))\n",
    "#plt.imshow(raw_target_array[0:100, 100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2 = UrbanGreenSegmentation2(out_channel=5)\n",
    "model2 = nn.DataParallel(model2)\n",
    "model2.load_state_dict(torch.load('/share/project/intern/Byeongchan/Data/Model/Segmentation/Categories_5/2022.8.8/Rotation_Activated_Dropout_Activated/Data/Model/Segmentation/Categories_5/2022.8.8/Rotation_Activated_Dropout_Activated/tmp_Rotation_Activated_Dropout_Activated/74.pth'))\n",
    "model2.to(device)\n",
    "region = input(\"Enter Region\") or 'N11'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_data_array_N11 ,raw_target_array_N11, OHE_target_array_N11 = dataprepare.prepare_raw_files(region)\n",
    "N11_prediction_dataset = dataprepare.TrainDataset4(raw_data_array_N11, OHE_target_array_N11, raw_target_array_N11, patch_size = patch_size, is_evaluating = True, train_ratio = train_ratio)\n",
    "N11_prediction_dataloader = DataLoader(N11_prediction_dataset, batch_size=2)\n",
    "reference_data = f'/share/project/intern/Byeongchan/Data/{region}/{region}_lidar.tif'\n",
    "result_path = legacytraining.save_result2(model2.to(device), dataloader=N11_prediction_dataloader, path=f'/share/project/intern/Byeongchan/Data/Model/Segmentation/{region}', reference_data=reference_data, patch_size=100, device = device, categories=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8bef1a0741f78125b97ca6015f4b21165d553afbb2c419d3dfb1350931d81372"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
