{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Type\n",
    "from torch import nn\n",
    "from torch.optim import optimizer\n",
    "import rasterio\n",
    "import zipfile\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "from torchvision import transforms as transforms\n",
    "import shutil\n",
    "import torchmetrics\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import sklearn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# --- GPU selection --- #\n",
    "gpus = 7 # slot number (e.g., 3), no gpu use -> write just ' '\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpus)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(array : Type[np.ndarray], dim = 0):\n",
    "    min = np.min(array, axis=dim)\n",
    "    max = np.max(array, axis=dim)\n",
    "    array = (array-min)/(max-min)\n",
    "    return array\n",
    "\n",
    "def log_minmax(array : Type[np.ndarray], dim = 0):\n",
    "    min = np.min(array, axis=dim)\n",
    "    array = array - min + 1\n",
    "    array = np.log(array)\n",
    "    max = np.max(array, axis=dim)\n",
    "    array = (array)/(max)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(model: Type[nn.Module], dataloader : Type[DataLoader], path:str, description:str = '', reference_data:str = '', patch_size:int = 60, now = datetime.datetime.now()):\n",
    "    best_model = model\n",
    "    os.makedirs(os.path.join(path,f'{now.year}.{now.month}.{now.day}/', f'{description}/','tmp/'), exist_ok=True)\n",
    "    zipped_results = zipfile.ZipFile(os.path.join(path,f'{now.year}.{now.month}.{now.day}/',f'{description}/','RESULT_{0:0=2d}:{1:0=2d}'.format(now.hour, now.minute)+f'_{description}.zip'), 'w')\n",
    "    prediction = np.zeros((60,60,7))\n",
    "    for i, (data, index_OHE, index) in enumerate(dataloader):\n",
    "        prediction[i, :, :] = best_model(data).detach().numpy()\n",
    "    prediction_expanded = np.zeros((7,2400,2400))\n",
    "    for i in range(60):\n",
    "        for j in range(60):\n",
    "            for k in range(7):\n",
    "                prediction_expanded[k,i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size] = prediction[i,j,k]\n",
    "\n",
    "    reference_image = rasterio.open(reference_data)\n",
    "    layer_index = [1,2,7,8,9,10,11]\n",
    "\n",
    "    for i in range(prediction_expanded.shape[0]):\n",
    "        print('a') \n",
    "        processed_tiff = rasterio.open(\n",
    "            os.path.join(path,f'{now.year}.{now.month}.{now.day}/',f'{description}/', 'tmp/', f'Result_{layer_index[i]}_{description}.tif'),\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=prediction_expanded.shape[1],\n",
    "            width=prediction_expanded.shape[2],\n",
    "            count=1,\n",
    "            dtype=prediction_expanded.dtype,\n",
    "            crs=reference_image.crs,\n",
    "            transform=reference_image.transform,\n",
    "        )\n",
    "        print('b')\n",
    "        processed_tiff.write(prediction_expanded[i,:,:],1)\n",
    "        processed_tiff.close()\n",
    "        print('c')\n",
    "        zipped_results.write(os.path.join(path,f'{now.year}.{now.month}.{now.day}/',f'{description}/', 'tmp/', f'Result_{layer_index[i]}_{description}.tif'), f'Result_{layer_index[i]}_{description}.tif')\n",
    "\n",
    "    zipped_results.close()\n",
    "    return os.path.join(path,f'{now.year}.{now.month}.{now.day}/',f'{description}/','RESULT_{0:0=2d}:{1:0=2d}'.format(now.hour, now.minute)+f'_{description}.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, device, num_epochs=13, train_rate: float = 0.8, batch_size: int = 60, path:str = '../Data/N12/Model/', description:str = 'no_description', reference_data:str = ''): \n",
    "    train_loss_history = []\n",
    "    valid_loss_history = []\n",
    "\n",
    "    patch_size = dataloaders['Train'].dataset.data.shape[-1]\n",
    "    training_patches = len(dataloaders['Train'].dataset)\n",
    "    validating_patches = len(dataloaders['Validation'].dataset)\n",
    "    print(f'Training Patches : {training_patches}\\nValidating Patches : {validating_patches}')\n",
    "\n",
    "    best_model_epoch = 0\n",
    "    least_valid_loss = 100\n",
    "    now = datetime.datetime.now()\n",
    "    os.makedirs(os.path.join(path,f'{now.year}.{now.month}.{now.day}/',f'{description}/', 'tmp/'), exist_ok=True)\n",
    "    zipped_model = zipfile.ZipFile(os.path.join(path,f'{now.year}.{now.month}.{now.day}/',f'{description}/', '{0:0=2d}:{1:0=2d}'.format(now.hour, now.minute)+f'_{description}'+'.zip'), 'w')\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        train_running_loss = 0.0\n",
    "        valid_running_loss = 0.0\n",
    "\n",
    "        for state in ['Train', 'Validation']:\n",
    "            for i, (inputs, labels_OHE, labels) in enumerate(dataloaders[state]):\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                model.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if state == 'Train':\n",
    "                    model.train()\n",
    "                    train_loss = criterion(outputs, labels)\n",
    "                    train_loss.backward()\n",
    "                    train_running_loss += train_loss.item() * inputs.size(0)\n",
    "                \n",
    "                if state == 'Validation':\n",
    "                    model.eval()\n",
    "                    valid_loss = criterion(outputs, labels)\n",
    "                    valid_running_loss += valid_loss.item() * inputs.size(0)\n",
    "\n",
    "                optimizer.step()\n",
    "                #valid_running_similarity += metric(outputs, labels)\n",
    "                #print('validating')\n",
    "                \n",
    "                #print(f'{i}th batch')\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        #print(f'Memory after a training : {torch.cuda.memory_allocated()/1024/1024}')\n",
    "\n",
    "        epoch_train_loss = train_running_loss / training_patches\n",
    "        epoch_valid_loss = valid_running_loss / validating_patches\n",
    "        scheduler.step(epoch_valid_loss)\n",
    "\n",
    "        print(f'Valid loss: {epoch_valid_loss} | Train loss: {epoch_train_loss}')\n",
    "\n",
    "\n",
    "        if epoch_valid_loss < least_valid_loss:\n",
    "            least_valid_loss = epoch_valid_loss\n",
    "            best_model_epoch = epoch\n",
    "\n",
    "        train_loss_history.append(epoch_train_loss)      \n",
    "        valid_loss_history.append(epoch_valid_loss)\n",
    "\n",
    "        torch.save(model.state_dict(), os.path.join(path,f'{now.year}.{now.month}.{now.day}/',f'{description}/','tmp/', '{0:0=2d}.pth'.format(epoch)))\n",
    "        zipped_model.write(os.path.join(path,f'{now.year}.{now.month}.{now.day}/',f'{description}/','tmp/', '{0:0=2d}.pth'.format(epoch)))\n",
    "\n",
    "    plt.figure(figsize=(20,8))\n",
    "    plt.plot(train_loss_history, 'r-')\n",
    "    plt.plot(valid_loss_history, 'bo')\n",
    "    plt.savefig(os.path.join(path,f'{now.year}.{now.month}.{now.day}/',f'{description}/','tmp/', 'Tendency.png'), dpi=300)\n",
    "    zipped_model.write(os.path.join(path,f'{now.year}.{now.month}.{now.day}/',f'{description}/','tmp/', 'Tendency.png'))\n",
    "    zipped_model.writestr('README.txt', f'{description}\\nThe best Model : #{best_model_epoch}th model with loss {least_valid_loss}\\nOptimizer : {optimizer}\\nLoss function : {criterion}\\nBatch size : {batch_size}\\nScheduler : {scheduler}\\nPatch size : {patch_size}\\nTotal epochs : {num_epochs}\\nModel information :\\n{model.modules}')\n",
    "    \n",
    "    print('Best loss: {:4f}, in Epoch #{:0=3d}'.format(least_valid_loss, best_model_epoch))    \n",
    "    zipped_model.close()\n",
    "    shutil.copy(src=os.path.join(path,f'{now.year}.{now.month}.{now.day}/',f'{description}/', 'tmp/', '{0:0=2d}.pth'.format(epoch)), dst=os.path.join(path,f'{now.year}.{now.month}.{now.day}/', 'Best_Model_Parameters_of_{0:0=2d}:{1:0=2d}'.format(now.hour, now.minute)+f'_{description}'+'.pth'))\n",
    "    print('Model information is saved in '+os.path.join(path,f'{now.year}.{now.month}.{now.day}/',f'{description}/', '{0:0=2d}:{1:0=2d}'.format(now.hour, now.minute)+f'_{description}'+'.zip'))\n",
    "\n",
    "    '''model.load_state_dict(torch.load(os.path.join(path,f'{now.year}.{now.month}.{now.day}/',f'{description}/','tmp/', '{0:0=2d}.pth'.format(best_model_epoch))))\n",
    "    result_path = save_result(model = model.to('cpu'), dataloader=dataloaders['Prediction'], path=path, description=description, reference_data=reference_data, patch_size=patch_size, now=now)\n",
    "    print('Model result is saved in '+ result_path)'''\n",
    "    \n",
    "    shutil.rmtree(os.path.join(path,f'{now.year}.{now.month}.{now.day}/',f'{description}/','tmp/'))\n",
    "    best_model_path = os.path.join(path,f'{now.year}.{now.month}.{now.day}/', 'Best_Model_Parameters_of_{0:0=2d}:{1:0=2d}'.format(now.hour, now.minute)+f'_{description}'+'.pth')\n",
    "    return best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_raw_files():\n",
    "    '''if os.path.exists('../Data/N12/np/train_array.npy') and os.path.exists('../Data/N12/np/target_array_OHE.npy'):\n",
    "        train_array = np.load('../Data/N12/np/train_array.npy')\n",
    "        target_array = np.load('../Data/N12/np/target_array_OHE.npy')\n",
    "    else:'''\n",
    "    lidar_image = rasterio.open('../Data/N12/N12_lidar.tif').read()\n",
    "    lidar_array = np.array(lidar_image)\n",
    "    lidar_array = log_minmax(lidar_array, dim=(0,1))\n",
    "\n",
    "    lidar_1n_image = rasterio.open('../Data/N12/N12_lidar_1n.tif').read()\n",
    "    lidar_1n_array = np.array(lidar_1n_image)\n",
    "    lidar_1n_array = log_minmax(lidar_1n_array, dim=(0,1))\n",
    "\n",
    "    lidar_nt_image = rasterio.open('../Data/N12/N12_lidar_nt.tif').read()\n",
    "    lidar_nt_array = np.array(lidar_nt_image)\n",
    "    lidar_nt_array = log_minmax(lidar_nt_array, dim=(0,1))\n",
    "\n",
    "    RGB2020_image = rasterio.open('../Data/N12/N12_RGB2020.tif').read()\n",
    "    RGB2020_array = np.array(RGB2020_image)\n",
    "\n",
    "    train_array = np.stack([lidar_array, lidar_1n_array, lidar_nt_array]).squeeze()\n",
    "    train_array = np.concatenate((train_array,RGB2020_array))\n",
    "    target_image = rasterio.open('../Data/N12/N12_newlc.tif').read()\n",
    "    target_array = np.array(target_image, dtype=int).squeeze()\n",
    "    target_array = np.where(target_array == 1, 0, target_array)\n",
    "    target_array = np.where(target_array == 2, 1, target_array)\n",
    "    target_array = np.where(target_array == 7, 2, target_array)\n",
    "    target_array = np.where(target_array == 8, 3, target_array)\n",
    "    target_array = np.where(target_array == 9, 4, target_array)\n",
    "    target_array = np.where(target_array == 10, 5, target_array)\n",
    "    target_array = np.where(target_array == 11, 6, target_array)\n",
    "\n",
    "    target_array_OHE = np.zeros(shape=(7,2400,2400))\n",
    "    num = np.unique(target_array)\n",
    "    num = num.shape[0]\n",
    "    encoded_target_array = np.eye(num)[target_array]\n",
    "    for i in range(encoded_target_array.shape[-1]):\n",
    "        target_array_OHE[i,:,:]=encoded_target_array[:,:,i]\n",
    "\n",
    "    return train_array, target_array.astype(int), target_array_OHE.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset2(Dataset):\n",
    "    def __init__(self, data_array : Type[np.ndarray], target_array_OHE : Type[np.ndarray], target_array_RAW : Type[np.ndarray], patch_size : int, is_evaluating : bool = False, is_validating : bool = False, rotate : bool = False, train_ratio : float = 0.8):\n",
    "        self.is_validating = is_validating\n",
    "        self.is_evaluating = is_evaluating\n",
    "        seed = 386579\n",
    "\n",
    "        #print(f'Data shape: {data_array.shape} | Target shape: {target_array.shape}')\n",
    "\n",
    "        self.data = np.zeros(((data_array.shape[1]//patch_size) * (data_array.shape[2]//patch_size), data_array.shape[0], patch_size, patch_size))\n",
    "\n",
    "        for i in range(0,data_array.shape[1]//patch_size):\n",
    "            for j in range(0,data_array.shape[2]//patch_size):\n",
    "                self.data[data_array.shape[1]//patch_size*i+j,:,:,:] = data_array[:,i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n",
    "\n",
    "        self.label_OHE = np.zeros(((data_array.shape[1]//patch_size) * (data_array.shape[2]//patch_size), target_array_OHE.shape[0] ,patch_size, patch_size), dtype=float)\n",
    "        for k in range(0,data_array.shape[1]//patch_size):\n",
    "            for l in range(0,data_array.shape[2]//patch_size):\n",
    "                self.label_OHE[data_array.shape[1]//patch_size*k+l,:,:,:] = target_array_OHE[:,i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n",
    "\n",
    "        self.label_RAW = np.zeros(((data_array.shape[1]//patch_size) * (data_array.shape[2]//patch_size),data_array.shape[0]+1))\n",
    "        for k in range(0,data_array.shape[1]//patch_size):\n",
    "            for l in range(0,data_array.shape[2]//patch_size):\n",
    "                self.label_RAW[data_array.shape[1]//patch_size*k+l,:] = np.bincount(target_array_RAW[k*patch_size:(k+1)*patch_size, l*patch_size:(l+1)*patch_size].reshape(-1), minlength=7)/(patch_size*patch_size)\n",
    "\n",
    "\n",
    "        if not is_evaluating:\n",
    "            if rotate:\n",
    "                for i in range(2):\n",
    "                    rotated_data = np.rot90(self.data, k=i+1, axes=(-2, -1))\n",
    "                    self.data = np.concatenate((self.data, rotated_data), axis=0)\n",
    "                    rotated_label_OHE = np.rot90(self.label_OHE, k=i+1, axes=(-2, -1))\n",
    "                    rotated_label_RAW = self.label_RAW\n",
    "                    self.label_OHE = np.concatenate((self.label_OHE, rotated_label_OHE), axis=0)\n",
    "                    self.label_RAW = np.concatenate((self.label_RAW, rotated_label_RAW), axis=0)\n",
    "\n",
    "        train_size = int(self.data.shape[0]*train_ratio)\n",
    "        index_array = np.random.RandomState(seed=seed).permutation(self.data.shape[0])\n",
    "        self.train_index = index_array[0:train_size]\n",
    "        self.valid_index = index_array[train_size:index_array.shape[0]]\n",
    "        \n",
    "        self.data = torch.as_tensor(self.data).float()\n",
    "        self.label_OHE = torch.as_tensor(self.label_OHE).float()\n",
    "        self.label_RAW = torch.as_tensor(self.label_RAW).float()\n",
    "\n",
    "        self.data[:,3:6,:,:] = self.data[:,3:6,:,:]/255\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.is_evaluating:\n",
    "            return self.data.shape[0]\n",
    "\n",
    "        if self.is_validating:\n",
    "            return self.valid_index.shape[0]\n",
    "        else:\n",
    "            return self.train_index.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_evaluating:\n",
    "            sample = torch.as_tensor(self.data[idx,:,:,:]).float()\n",
    "            label_OHE = torch.as_tensor(self.label_OHE[idx,:]).float()\n",
    "            label_RAW = torch.as_tensor(self.label_RAW[idx,:]).float()\n",
    "            return sample, label_OHE, label_RAW\n",
    "        \n",
    "        if self.is_validating:\n",
    "            sample = torch.as_tensor(self.data[self.valid_index[idx],:,:,:]).float()\n",
    "            label_OHE = torch.as_tensor(self.label_OHE[self.valid_index[idx],:]).float()\n",
    "            label_RAW = torch.as_tensor(self.label_RAW[self.valid_index[idx],:]).float()\n",
    "        else:\n",
    "            sample = torch.as_tensor(self.data[self.train_index[idx],:,:,:]).float()\n",
    "            label_OHE = torch.as_tensor(self.label_OHE[self.train_index[idx],:]).float()\n",
    "            label_RAW = torch.as_tensor(self.label_RAW[self.train_index[idx],:]).float()\n",
    "\n",
    "        return sample, label_OHE, label_RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_padding(inputs, kernel_size, dilation):\n",
    "    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n",
    "    pad_total = kernel_size_effective - 1\n",
    "    pad_beg = pad_total // 2\n",
    "    pad_end = pad_total - pad_beg\n",
    "    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))\n",
    "    return padded_inputs\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, dilation=1, bias=False, BatchNorm=None):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, 0, dilation,\n",
    "                               groups=inplanes, bias=bias)\n",
    "        self.bn = BatchNorm(inplanes)\n",
    "        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = fixed_padding(x, self.conv1.kernel_size[0], dilation=self.conv1.dilation[0])\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBN(nn.Module):\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride, dilation = 1, affine=True, fix_padding = False):\n",
    "        super(ConvBN, self).__init__()\n",
    "        self.fix_padding = fix_padding\n",
    "        self.conv2d = nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, groups=C_in, bias=False, dilation=dilation)\n",
    "        self.pointwise = nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False)\n",
    "        self.batchnorm = nn.BatchNorm2d(C_in, affine=affine)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.fix_padding:\n",
    "           x = fixed_padding(x, self.conv2d.kernel_size[0], dilation=self.conv2d.dilation[0])\n",
    "        x = self.conv2d(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"class Block(nn.Module):\\n    def __init__(self, C_in, C_out, reps, stride = 1, dilation = 1, grow_first = True, start_with_relu = True, is_last = False):\\n        super(Block, self).__init__()\\n        \\n        if C_in!=C_out or stride!=1:\\n            self.skip = ConvBN(C_in, C_out, kernel_size=1, stride=stride)\\n        else:\\n            self.skip = None\\n\\n        self.relu = nn.ReLU(inplace=True)\\n        rep = []\\n\\n        filters = C_in\\n        if grow_first:\\n            rep.append(self.relu)\\n            rep.append(ConvBN(C_in, C_out, kernel_size=3, stride=1, dilation = dilation))\\n            filters = C_out\\n\\n        for i in range(reps-1):\\n            rep.append(self.relu)\\n            rep.append(ConvBN(filters, filters, 3, stride=1, dilation = dilation))\\n\\n        if not grow_first:\\n            rep.append(self.relu)\\n            rep.append(ConvBN(C_in, C_out, 3, stride=1, dilation = dilation))\\n\\n        if stride != 1:\\n            rep.append(self.relu)\\n            rep.append(ConvBN(C_out, C_out, 3, stride=2, dilation = dilation))\\n\\n        if stride == 1 and is_last:\\n            rep.append(self.relu)\\n            rep.append(ConvBN(C_out, C_out, 3, 1))\\n\\n        if not start_with_relu:\\n            rep = rep[1:]\\n\\n        self.rep = nn.Sequential(*rep)\\n\\n    def forward(self, x):\\n        #print(f'Shape before residual : {x.shape}')\\n\\n        if self.skip is not None:\\n            #print('trying residual')\\n            skip = self.skip(x)\\n            #print('succeded residual')\\n        else:\\n            skip = x\\n        x = self.rep(x)\\n\\n        #print(f'Shape after residual : {x.shape}')\\n        x += skip\\n        return x\\n\\n    \\n\\n        \""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class Block(nn.Module):\n",
    "    def __init__(self, C_in, C_out, reps, stride = 1, dilation = 1, grow_first = True, start_with_relu = True, is_last = False):\n",
    "        super(Block, self).__init__()\n",
    "        \n",
    "        if C_in!=C_out or stride!=1:\n",
    "            self.skip = ConvBN(C_in, C_out, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.skip = None\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        rep = []\n",
    "\n",
    "        filters = C_in\n",
    "        if grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(ConvBN(C_in, C_out, kernel_size=3, stride=1, dilation = dilation))\n",
    "            filters = C_out\n",
    "\n",
    "        for i in range(reps-1):\n",
    "            rep.append(self.relu)\n",
    "            rep.append(ConvBN(filters, filters, 3, stride=1, dilation = dilation))\n",
    "\n",
    "        if not grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(ConvBN(C_in, C_out, 3, stride=1, dilation = dilation))\n",
    "\n",
    "        if stride != 1:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(ConvBN(C_out, C_out, 3, stride=2, dilation = dilation))\n",
    "\n",
    "        if stride == 1 and is_last:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(ConvBN(C_out, C_out, 3, 1))\n",
    "\n",
    "        if not start_with_relu:\n",
    "            rep = rep[1:]\n",
    "\n",
    "        self.rep = nn.Sequential(*rep)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f'Shape before residual : {x.shape}')\n",
    "\n",
    "        if self.skip is not None:\n",
    "            #print('trying residual')\n",
    "            skip = self.skip(x)\n",
    "            #print('succeded residual')\n",
    "        else:\n",
    "            skip = x\n",
    "        x = self.rep(x)\n",
    "\n",
    "        #print(f'Shape after residual : {x.shape}')\n",
    "        x += skip\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class Xception(nn.Module):\\n    def __init__(self, output_stride=16):\\n        super(Xception, self).__init__()\\n        if output_stride == 16:\\n            entry_block3_stride = 2\\n            middle_block_dilation = 1\\n            exit_block_dilations = (1, 2)\\n        elif output_stride == 8:\\n            entry_block3_stride = 1\\n            middle_block_dilation = 2\\n            exit_block_dilations = (2, 4)\\n        else:\\n            raise NotImplementedError\\n\\n        self.entry_flow = nn.Sequential(\\n            nn.Conv2d(6, 32, kernel_size=3, stride=2, padding=1, bias=False),\\n            nn.BatchNorm2d(32),\\n            nn.ReLU(),\\n            nn.Conv2d(32,64, kernel_size=3, stride=1, padding=1, bias=False),\\n            nn.BatchNorm2d(64),\\n            nn.ReLU(),\\n            Block(64, 128, reps=2, stride=2, start_with_relu=False),\\n            Block(128, 256, reps=2, stride=2, start_with_relu=False, grow_first=True),\\n            Block(256, 728, reps=2, stride=entry_block3_stride, start_with_relu=True, grow_first=True, is_last=True)\\n        )\\n\\n        self.middle_flow = nn.Sequential(\\n            Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True),\\n            Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True),\\n            Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True),\\n            Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True),\\n            Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True),\\n            Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True),\\n            Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True),\\n            Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\\n        )\\n\\n        self.exit_flow = nn.Sequential(\\n            Block(728, 1024, reps=2, stride=1, dilation=exit_block_dilations[0], start_with_relu=True, grow_first=False, is_last=True),\\n            ConvBN(1024, 1536, 3, stride=1, dilation=exit_block_dilations[1]),\\n            ConvBN(1536, 1536, 3, stride=1, dilation=exit_block_dilations[1]),\\n            ConvBN(1536, 2048, 3, stride=1, dilation=exit_block_dilations[1])\\n        )\\n\\n    def forward(self, x):\\n        x = self.entry_flow(x)\\n        x = self.middle_flow(x)\\n        x = self.exit_flow(x)\\n        return x'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class Xception(nn.Module):\n",
    "    def __init__(self, output_stride=16):\n",
    "        super(Xception, self).__init__()\n",
    "        if output_stride == 16:\n",
    "            entry_block3_stride = 2\n",
    "            middle_block_dilation = 1\n",
    "            exit_block_dilations = (1, 2)\n",
    "        elif output_stride == 8:\n",
    "            entry_block3_stride = 1\n",
    "            middle_block_dilation = 2\n",
    "            exit_block_dilations = (2, 4)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.entry_flow = nn.Sequential(\n",
    "            nn.Conv2d(6, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            Block(64, 128, reps=2, stride=2, start_with_relu=False),\n",
    "            Block(128, 256, reps=2, stride=2, start_with_relu=False, grow_first=True),\n",
    "            Block(256, 728, reps=2, stride=entry_block3_stride, start_with_relu=True, grow_first=True, is_last=True)\n",
    "        )\n",
    "\n",
    "        self.middle_flow = nn.Sequential(\n",
    "            Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True),\n",
    "            Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True),\n",
    "            Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True),\n",
    "            Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True),\n",
    "            Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True),\n",
    "            Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True),\n",
    "            Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True),\n",
    "            Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n",
    "        )\n",
    "\n",
    "        self.exit_flow = nn.Sequential(\n",
    "            Block(728, 1024, reps=2, stride=1, dilation=exit_block_dilations[0], start_with_relu=True, grow_first=False, is_last=True),\n",
    "            ConvBN(1024, 1536, 3, stride=1, dilation=exit_block_dilations[1]),\n",
    "            ConvBN(1536, 1536, 3, stride=1, dilation=exit_block_dilations[1]),\n",
    "            ConvBN(1536, 2048, 3, stride=1, dilation=exit_block_dilations[1])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.entry_flow(x)\n",
    "        x = self.middle_flow(x)\n",
    "        x = self.exit_flow(x)\n",
    "        return x'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanGreenRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UrbanGreenRegression, self).__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            ConvBN(6,32,3,1),#48\n",
    "            nn.ReLU(),\n",
    "            ConvBN(32,32,3,1),#46\n",
    "            nn.ReLU(),\n",
    "            ConvBN(32,32,3,1),#44\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)#22\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            ConvBN(32,64,3,1),#20\n",
    "            nn.ReLU(),\n",
    "            ConvBN(64,64,3,1),#18\n",
    "            nn.ReLU(),\n",
    "            ConvBN(64,64,3,1),#16\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)#8\n",
    "        )\n",
    "\n",
    "        self.fc_block_1 = nn.Sequential(\n",
    "            nn.Linear(in_features=4096, out_features=1024, bias=False),\n",
    "            nn.BatchNorm1d(num_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=1024, bias=False),\n",
    "            nn.BatchNorm1d(num_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=1024, bias=False),\n",
    "            nn.BatchNorm1d(num_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=1024, bias=False),\n",
    "            nn.BatchNorm1d(num_features=1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_block_2 = nn.Sequential(\n",
    "            nn.Linear(in_features=1024, out_features=256, bias=False),\n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=64, bias=False),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,7, False),\n",
    "            nn.BatchNorm1d(7)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        #print(x.shape)\n",
    "        x = self.fc_block_1(x)\n",
    "        x = self.fc_block_2(x)\n",
    "        return torch.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_array ,raw_target_array, OHE_target_array = prepare_raw_files()\n",
    "batch_size = 64\n",
    "patch_size = 50\n",
    "train_ratio = 0.8\n",
    "rotate_training_data = True\n",
    "Datasets_NON_OHE = {\n",
    "    'Train' : TrainDataset2(raw_data_array, OHE_target_array, raw_target_array, patch_size = patch_size, rotate = rotate_training_data, train_ratio = train_ratio),\n",
    "    'Validation' : TrainDataset2(raw_data_array, OHE_target_array, raw_target_array, patch_size = patch_size, is_validating = True, rotate = rotate_training_data, train_ratio = train_ratio),\n",
    "    'Prediction' : TrainDataset2(raw_data_array, OHE_target_array, raw_target_array, patch_size = patch_size, is_evaluating = True, train_ratio = train_ratio)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataloaders_NON_OHE = {\n",
    "    'Train' : DataLoader(Datasets_NON_OHE['Train'], batch_size=64),\n",
    "    'Validation' : DataLoader(Datasets_NON_OHE['Validation'], batch_size=64),\n",
    "    'Prediction' : DataLoader(Datasets_NON_OHE['Prediction'], batch_size=64)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 8, 8])\n",
      "torch.Size([2, 4096])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [64, 6, 48, 48]              54\n",
      "       BatchNorm2d-2            [64, 6, 48, 48]              12\n",
      "            Conv2d-3           [64, 32, 48, 48]             192\n",
      "            ConvBN-4           [64, 32, 48, 48]               0\n",
      "              ReLU-5           [64, 32, 48, 48]               0\n",
      "            Conv2d-6           [64, 32, 46, 46]             288\n",
      "       BatchNorm2d-7           [64, 32, 46, 46]              64\n",
      "            Conv2d-8           [64, 32, 46, 46]           1,024\n",
      "            ConvBN-9           [64, 32, 46, 46]               0\n",
      "             ReLU-10           [64, 32, 46, 46]               0\n",
      "           Conv2d-11           [64, 32, 44, 44]             288\n",
      "      BatchNorm2d-12           [64, 32, 44, 44]              64\n",
      "           Conv2d-13           [64, 32, 44, 44]           1,024\n",
      "           ConvBN-14           [64, 32, 44, 44]               0\n",
      "             ReLU-15           [64, 32, 44, 44]               0\n",
      "        MaxPool2d-16           [64, 32, 22, 22]               0\n",
      "           Conv2d-17           [64, 32, 20, 20]             288\n",
      "      BatchNorm2d-18           [64, 32, 20, 20]              64\n",
      "           Conv2d-19           [64, 64, 20, 20]           2,048\n",
      "           ConvBN-20           [64, 64, 20, 20]               0\n",
      "             ReLU-21           [64, 64, 20, 20]               0\n",
      "           Conv2d-22           [64, 64, 18, 18]             576\n",
      "      BatchNorm2d-23           [64, 64, 18, 18]             128\n",
      "           Conv2d-24           [64, 64, 18, 18]           4,096\n",
      "           ConvBN-25           [64, 64, 18, 18]               0\n",
      "             ReLU-26           [64, 64, 18, 18]               0\n",
      "           Conv2d-27           [64, 64, 16, 16]             576\n",
      "      BatchNorm2d-28           [64, 64, 16, 16]             128\n",
      "           Conv2d-29           [64, 64, 16, 16]           4,096\n",
      "           ConvBN-30           [64, 64, 16, 16]               0\n",
      "             ReLU-31           [64, 64, 16, 16]               0\n",
      "        MaxPool2d-32             [64, 64, 8, 8]               0\n",
      "           Linear-33                 [64, 1024]       4,194,304\n",
      "      BatchNorm1d-34                 [64, 1024]           2,048\n",
      "             ReLU-35                 [64, 1024]               0\n",
      "           Linear-36                 [64, 1024]       1,048,576\n",
      "      BatchNorm1d-37                 [64, 1024]           2,048\n",
      "             ReLU-38                 [64, 1024]               0\n",
      "           Linear-39                 [64, 1024]       1,048,576\n",
      "      BatchNorm1d-40                 [64, 1024]           2,048\n",
      "             ReLU-41                 [64, 1024]               0\n",
      "           Linear-42                 [64, 1024]       1,048,576\n",
      "      BatchNorm1d-43                 [64, 1024]           2,048\n",
      "             ReLU-44                 [64, 1024]               0\n",
      "           Linear-45                  [64, 256]         262,144\n",
      "      BatchNorm1d-46                  [64, 256]             512\n",
      "             ReLU-47                  [64, 256]               0\n",
      "           Linear-48                   [64, 64]          16,384\n",
      "      BatchNorm1d-49                   [64, 64]             128\n",
      "             ReLU-50                   [64, 64]               0\n",
      "           Linear-51                    [64, 7]             448\n",
      "      BatchNorm1d-52                    [64, 7]              14\n",
      "================================================================\n",
      "Total params: 7,642,864\n",
      "Trainable params: 7,642,864\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.66\n",
      "Forward/backward pass size (MB): 594.73\n",
      "Params size (MB): 29.16\n",
      "Estimated Total Size (MB): 627.54\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model_for_summary = UrbanGreenRegression()\n",
    "model_for_summary.to(device)\n",
    "summary(model_for_summary, input_size=(6,50,50), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Patches : 7372\n",
      "Validating Patches : 1844\n",
      "Epoch 0/99\n",
      "----------\n",
      "Valid loss: 0.033291645024836454 | Train loss: 0.05425461253926473\n",
      "Epoch 1/99\n",
      "----------\n",
      "Valid loss: 0.029653328747656235 | Train loss: 0.029041698509731245\n",
      "Epoch 2/99\n",
      "----------\n",
      "Valid loss: 0.028639159393607925 | Train loss: 0.020652578709565196\n",
      "Epoch 3/99\n",
      "----------\n",
      "Valid loss: 0.025827271314317647 | Train loss: 0.016121469875691512\n",
      "Epoch 4/99\n",
      "----------\n",
      "Valid loss: 0.02258899927527165 | Train loss: 0.013687958245024659\n",
      "Epoch 5/99\n",
      "----------\n",
      "Valid loss: 0.022210540140851717 | Train loss: 0.011740701006269623\n",
      "Epoch 6/99\n",
      "----------\n",
      "Valid loss: 0.021860769067016186 | Train loss: 0.009965909934515255\n",
      "Epoch 7/99\n",
      "----------\n",
      "Valid loss: 0.02147361095550262 | Train loss: 0.008390276861082454\n",
      "Epoch 8/99\n",
      "----------\n",
      "Valid loss: 0.022062075362740782 | Train loss: 0.007229629284497583\n",
      "Epoch 9/99\n",
      "----------\n",
      "Valid loss: 0.021832878125625423 | Train loss: 0.00660079700050636\n",
      "Epoch 10/99\n",
      "----------\n",
      "Valid loss: 0.022181831227530625 | Train loss: 0.006015225070540309\n",
      "Epoch 11/99\n",
      "----------\n",
      "Valid loss: 0.022648958768089526 | Train loss: 0.005650964209823038\n",
      "Epoch 12/99\n",
      "----------\n",
      "Valid loss: 0.02190799793782048 | Train loss: 0.005323032836282724\n",
      "Epoch 13/99\n",
      "----------\n",
      "Valid loss: 0.022877197357100674 | Train loss: 0.005041016821107234\n",
      "Epoch 14/99\n",
      "----------\n",
      "Valid loss: 0.022105108898076473 | Train loss: 0.004830961177813832\n",
      "Epoch 15/99\n",
      "----------\n",
      "Valid loss: 0.01932542483017269 | Train loss: 0.004293550324600495\n",
      "Epoch 16/99\n",
      "----------\n",
      "Valid loss: 0.01816254574603476 | Train loss: 0.004073481793665467\n",
      "Epoch 17/99\n",
      "----------\n",
      "Valid loss: 0.018284588947148746 | Train loss: 0.00378123643604419\n",
      "Epoch 18/99\n",
      "----------\n",
      "Valid loss: 0.018590018427940874 | Train loss: 0.0035388622105998863\n",
      "Epoch 19/99\n",
      "----------\n",
      "Valid loss: 0.020312847715040865 | Train loss: 0.003384409039910444\n",
      "Epoch 20/99\n",
      "----------\n",
      "Valid loss: 0.02039185510200818 | Train loss: 0.0032423729158086957\n",
      "Epoch 21/99\n",
      "----------\n",
      "Valid loss: 0.018928769422359683 | Train loss: 0.0031155922335920714\n",
      "Epoch 22/99\n",
      "----------\n",
      "Valid loss: 0.01922307906514781 | Train loss: 0.0030326199731968923\n",
      "Epoch 23/99\n",
      "----------\n",
      "Valid loss: 0.01853545871108706 | Train loss: 0.0029494421120997818\n",
      "Epoch 24/99\n",
      "----------\n",
      "Valid loss: 0.01873602509223846 | Train loss: 0.0026849188318357835\n",
      "Epoch 25/99\n",
      "----------\n",
      "Valid loss: 0.018613094738694696 | Train loss: 0.002622131060918048\n",
      "Epoch 26/99\n",
      "----------\n",
      "Valid loss: 0.018926570345516317 | Train loss: 0.002536604714181175\n",
      "Epoch 27/99\n",
      "----------\n",
      "Valid loss: 0.018497813690995187 | Train loss: 0.002473890017849462\n",
      "Epoch 28/99\n",
      "----------\n",
      "Valid loss: 0.01865379045234585 | Train loss: 0.002406143940878073\n",
      "Epoch 29/99\n",
      "----------\n",
      "Valid loss: 0.018894361194683778 | Train loss: 0.0024095147251713936\n",
      "Epoch 30/99\n",
      "----------\n",
      "Valid loss: 0.01874696526781468 | Train loss: 0.0022843271086237034\n",
      "Epoch 31/99\n",
      "----------\n",
      "Valid loss: 0.01855361046685702 | Train loss: 0.0022027064431903404\n",
      "Epoch 32/99\n",
      "----------\n",
      "Valid loss: 0.018503102823413116 | Train loss: 0.0021429053823955302\n",
      "Epoch 33/99\n",
      "----------\n",
      "Valid loss: 0.018345557023930703 | Train loss: 0.002086840972720736\n",
      "Epoch 34/99\n",
      "----------\n",
      "Valid loss: 0.01861727830434699 | Train loss: 0.002041944428003427\n",
      "Epoch 35/99\n",
      "----------\n",
      "Valid loss: 0.018700353910342474 | Train loss: 0.0020535560773899835\n",
      "Epoch 36/99\n",
      "----------\n",
      "Valid loss: 0.018277030195758038 | Train loss: 0.001973688469040478\n",
      "Epoch 37/99\n",
      "----------\n",
      "Valid loss: 0.017859782203385473 | Train loss: 0.001904704474679301\n",
      "Epoch 38/99\n",
      "----------\n",
      "Valid loss: 0.017826906116497492 | Train loss: 0.001854014043871574\n",
      "Epoch 39/99\n",
      "----------\n",
      "Valid loss: 0.01782280531936121 | Train loss: 0.0017988968173641262\n",
      "Epoch 40/99\n",
      "----------\n",
      "Valid loss: 0.017877905421492334 | Train loss: 0.0017570046333664836\n",
      "Epoch 41/99\n",
      "----------\n",
      "Valid loss: 0.017908389043717478 | Train loss: 0.0017239881351077753\n",
      "Epoch 42/99\n",
      "----------\n",
      "Valid loss: 0.0179110652769142 | Train loss: 0.0016909942324755877\n",
      "Epoch 43/99\n",
      "----------\n",
      "Valid loss: 0.017896033391162528 | Train loss: 0.0016610224945905275\n",
      "Epoch 44/99\n",
      "----------\n",
      "Valid loss: 0.01792470172629439 | Train loss: 0.0016327195579632792\n",
      "Epoch 45/99\n",
      "----------\n",
      "Valid loss: 0.017956484707149147 | Train loss: 0.0016054921555224194\n",
      "Epoch 46/99\n",
      "----------\n",
      "Valid loss: 0.01803981204499004 | Train loss: 0.0015821252860218828\n",
      "Epoch 47/99\n",
      "----------\n",
      "Valid loss: 0.018059916681167618 | Train loss: 0.0015644515201684878\n",
      "Epoch 48/99\n",
      "----------\n",
      "Valid loss: 0.018065425573389596 | Train loss: 0.00154576426513677\n",
      "Epoch 49/99\n",
      "----------\n",
      "Valid loss: 0.018058344814724365 | Train loss: 0.0015279731625560682\n",
      "Epoch 50/99\n",
      "----------\n",
      "Valid loss: 0.01807413272026304 | Train loss: 0.0015110970112584567\n",
      "Epoch 51/99\n",
      "----------\n",
      "Valid loss: 0.018088849229079523 | Train loss: 0.0014949976101495442\n",
      "Epoch 52/99\n",
      "----------\n",
      "Valid loss: 0.01810962676500938 | Train loss: 0.0014804772034260165\n",
      "Epoch 53/99\n",
      "----------\n",
      "Valid loss: 0.018113312770061018 | Train loss: 0.0014692477860937237\n",
      "Epoch 54/99\n",
      "----------\n",
      "Valid loss: 0.018115973689971157 | Train loss: 0.001458200066233164\n",
      "Epoch 55/99\n",
      "----------\n",
      "Valid loss: 0.018117907745945478 | Train loss: 0.0014474129409019392\n",
      "Epoch 56/99\n",
      "----------\n",
      "Valid loss: 0.018121068371932572 | Train loss: 0.0014368696048566733\n",
      "Epoch 57/99\n",
      "----------\n",
      "Valid loss: 0.018124916083674367 | Train loss: 0.0014265043307520244\n",
      "Epoch 58/99\n",
      "----------\n",
      "Valid loss: 0.018123411660339206 | Train loss: 0.0014169278468206985\n",
      "Epoch 59/99\n",
      "----------\n",
      "Valid loss: 0.018125442521335505 | Train loss: 0.0014093846402434725\n",
      "Epoch 60/99\n",
      "----------\n",
      "Valid loss: 0.018128419521218266 | Train loss: 0.0014019171108263814\n",
      "Epoch 61/99\n",
      "----------\n",
      "Valid loss: 0.01813196504403738 | Train loss: 0.0013945028404399502\n",
      "Epoch 62/99\n",
      "----------\n",
      "Valid loss: 0.018135704928423216 | Train loss: 0.0013871437915319368\n",
      "Epoch 63/99\n",
      "----------\n",
      "Valid loss: 0.01813926361501217 | Train loss: 0.001379865652472751\n",
      "Epoch 64/99\n",
      "----------\n",
      "Valid loss: 0.018140548593697993 | Train loss: 0.0013730987724284648\n",
      "Epoch 65/99\n",
      "----------\n",
      "Valid loss: 0.018144592351872078 | Train loss: 0.0013677239941298654\n",
      "Epoch 66/99\n",
      "----------\n",
      "Valid loss: 0.01814799837832725 | Train loss: 0.00136238073191605\n",
      "Epoch 67/99\n",
      "----------\n",
      "Valid loss: 0.018151943743584472 | Train loss: 0.001357074478088827\n",
      "Epoch 68/99\n",
      "----------\n",
      "Valid loss: 0.0181559687258106 | Train loss: 0.0013518201686139712\n",
      "Epoch 69/99\n",
      "----------\n",
      "Valid loss: 0.018159304206095912 | Train loss: 0.0013466059548373026\n",
      "Epoch 70/99\n",
      "----------\n",
      "Valid loss: 0.018162444019750763 | Train loss: 0.0013417449117952992\n",
      "Epoch 71/99\n",
      "----------\n",
      "Valid loss: 0.018165913739781058 | Train loss: 0.0013378937234743273\n",
      "Epoch 72/99\n",
      "----------\n",
      "Valid loss: 0.01816894917661999 | Train loss: 0.0013340774580444161\n",
      "Epoch 73/99\n",
      "----------\n",
      "Valid loss: 0.018172273165879695 | Train loss: 0.001330288867783246\n",
      "Epoch 74/99\n",
      "----------\n",
      "Valid loss: 0.01817477601954063 | Train loss: 0.0013265276914353802\n",
      "Epoch 75/99\n",
      "----------\n",
      "Valid loss: 0.018177956932602116 | Train loss: 0.0013227958197865133\n",
      "Epoch 76/99\n",
      "----------\n",
      "Valid loss: 0.0181798431739502 | Train loss: 0.0013192943098037178\n",
      "Epoch 77/99\n",
      "----------\n",
      "Valid loss: 0.018181736667897075 | Train loss: 0.0013165149503393575\n",
      "Epoch 78/99\n",
      "----------\n",
      "Valid loss: 0.018183790951353092 | Train loss: 0.0013137545615823246\n",
      "Epoch 79/99\n",
      "----------\n",
      "Valid loss: 0.018185678000790177 | Train loss: 0.0013110097991029732\n",
      "Epoch 80/99\n",
      "----------\n",
      "Valid loss: 0.018187654804672698 | Train loss: 0.0013082733979977756\n",
      "Epoch 81/99\n",
      "----------\n",
      "Valid loss: 0.018189527255981965 | Train loss: 0.0013055512519582107\n",
      "Epoch 82/99\n",
      "----------\n",
      "Valid loss: 0.018190650467370957 | Train loss: 0.00130298814184992\n",
      "Epoch 83/99\n",
      "----------\n",
      "Valid loss: 0.01819227108178325 | Train loss: 0.0013009546458144074\n",
      "Epoch 84/99\n",
      "----------\n",
      "Valid loss: 0.018193645903804297 | Train loss: 0.0012989302182040009\n",
      "Epoch 85/99\n",
      "----------\n",
      "Valid loss: 0.018195094015457366 | Train loss: 0.0012969112134441569\n",
      "Epoch 86/99\n",
      "----------\n",
      "Valid loss: 0.01819633145022677 | Train loss: 0.0012948980498846724\n",
      "Epoch 87/99\n",
      "----------\n",
      "Valid loss: 0.018197731440179776 | Train loss: 0.0012928906984375087\n",
      "Epoch 88/99\n",
      "----------\n",
      "Valid loss: 0.018198514975397293 | Train loss: 0.0012909957383404579\n",
      "Epoch 89/99\n",
      "----------\n",
      "Valid loss: 0.01819971786840231 | Train loss: 0.0012894923022364045\n",
      "Epoch 90/99\n",
      "----------\n",
      "Valid loss: 0.01820075987896795 | Train loss: 0.0012879933752205448\n",
      "Epoch 91/99\n",
      "----------\n",
      "Valid loss: 0.018201809740118247 | Train loss: 0.0012864995270520773\n",
      "Epoch 92/99\n",
      "----------\n",
      "Valid loss: 0.018202847823371338 | Train loss: 0.001285006307418965\n",
      "Epoch 93/99\n",
      "----------\n",
      "Valid loss: 0.01820383536469781 | Train loss: 0.0012835179553422785\n",
      "Epoch 94/99\n",
      "----------\n",
      "Valid loss: 0.018204561533677086 | Train loss: 0.0012821085280531586\n",
      "Epoch 95/99\n",
      "----------\n",
      "Valid loss: 0.018205339969853 | Train loss: 0.00128099047842906\n",
      "Epoch 96/99\n",
      "----------\n",
      "Valid loss: 0.018206071734848613 | Train loss: 0.0012798749687497788\n",
      "Epoch 97/99\n",
      "----------\n",
      "Valid loss: 0.018206911044369032 | Train loss: 0.00127876081739664\n",
      "Epoch 98/99\n",
      "----------\n",
      "Valid loss: 0.01820754669525877 | Train loss: 0.0012776487728892116\n",
      "Epoch 99/99\n",
      "----------\n",
      "Valid loss: 0.018208344469004493 | Train loss: 0.0012765368525665486\n",
      "Best loss: 0.017823, in Epoch #039\n",
      "Model information is saved in /home/bcyoon/Byeongchan/Data/N12/Model/Segmentation/Regression/2022.7.25/patch_50/15:00_patch_50.zip\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIcAAAHSCAYAAAB2Cqt4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA3HUlEQVR4nO3de7xdd10n/M/vJE3atDltacutl6TQWnoAuR2rgqAjjrYP0HqpQ3migi803hjxQXisxHG0Y0d4BgGdgZHMUEEMAw6gTxhQRNGhw0jpKXW4tHZMb2kLvdhr2jRNk/zmj3WO5+T0JNkn2Xuvdc5+v1+v9Vp7/dbv7P3daXZ28unvUmqtAQAAAGA0jbVdAAAAAADtEQ4BAAAAjDDhEAAAAMAIEw4BAAAAjDDhEAAAAMAIEw4BAAAAjLCVbRcw38knn1zXr1/fdhkAAAAAy8Y111zzj7XWUxa617lwaP369Zmammq7DAAAAIBlo5Ry64HumVYGAAAAMMKEQwAAAAAjTDgEAAAAMMKEQwAAAAAjTDgEAAAAMMKEQwAAAAAjTDgEAAAAMMKEQwAAAAAjTDgEAAAAMMKEQwAAAAAjTDgEAAAAMMKEQwAAAAAjTDgEAAAAMMKEQwAAAAAjTDgEAAAAMMKEQwAAAAAjTDg0KLt2Jfff33YVAAAAAAclHBqUH/7h5Pu/v+0qAAAAAA5KODQo4+PJQw+1XQUAAADAQQmHBkU4BAAAACwBwqFBEQ4BAAAAS4BwaFDWrk127kz27Gm7EgAAAIADEg4Nyvh4c96xo906AAAAAA5CODQoM+GQqWUAAABAhwmHBkU4BAAAACwBwqFBMa0MAAAAWAKEQ4Ni5BAAAACwBAiHBkU4BAAAACwBwqFBEQ4BAAAAS4BwaFCEQwAAAMASIBwalOOOa87CIQAAAKDDhEODsmJFExAJhwAAAIAOEw4N0vi4cAgAAADoNOHQIAmHAAAAgI4TDg2ScAgAAADoOOHQIAmHAAAAgI4TDg2ScAgAAADoOOHQIAmHAAAAgI4TDg2ScAgAAADoOOHQIK1d24RDtbZdCQAAAMCChEODND7eBEOPPNJ2JQAAAAALEg4N0vh4cza1DAAAAOgo4dAgzYRDO3a0WwcAAADAAQiHBsnIIQAAAKDjhEODJBwCAAAAOk44NEjCIQAAAKDjegqHSinnl1JuKKVsK6VcusD91aWUj07fv6qUsn66fX0p5dFSyt9NH7/f5/q7TTgEAAAAdNzKQ3UopaxI8p4k/zzJ7UmuLqVsrbVeN6fb65PcX2s9q5RySZK3J3n19L0ba63P72/ZS4RwCAAAAOi4XkYOnZdkW631plrr7iQfSXLRvD4XJfng9OOPJXl5KaX0r8wlau3a5iwcAgAAADqql3Do1CS3zbm+fbptwT611j1JHkxy0vS9M0sp15ZS/nsp5aULvUApZWMpZaqUMnXPPfcs6g102urVzSEcAgAAADpq0AtSfzPJGbXWFyR5U5IPl1LG53eqtW6utU7WWidPOeWUAZc0ZOPjwiEAAACgs3oJh+5Icvqc69Om2xbsU0pZmeT4JPfWWh+rtd6bJLXWa5LcmORbjrToJUU4BAAAAHRYL+HQ1UnOLqWcWUpZleSSJFvn9dma5LXTjy9O8rlaay2lnDK9oHVKKc9IcnaSm/pT+hIhHAIAAAA67JC7ldVa95RS3pDkM0lWJLmi1vr1UsplSaZqrVuTvD/Jh0op25LclyZASpKXJbmslPJ4kn1JfrbWet8g3khnCYcAAACADjtkOJQktdZPJ/n0vLZfn/N4V5IfXeDnPp7k40dY49I2Pp7cdtuh+wEAAAC0YNALUmPkEAAAANBhwqFBEw4BAAAAHSYcGrS1a4VDAAAAQGcJhwZtfDzZvTt57LG2KwEAAAB4AuHQoI2PN+cdO9qtAwAAAGABwqFBmwmHTC0DAAAAOkg4NGjCIQAAAKDDhEODJhwCAAAAOkw4NGjCIQAAAKDDhEODJhwCAAAAOkw4NGjCIQAAAKDDhEODJhwCAAAAOkw4NGhr1iRjY8IhAAAAoJOEQ4NWSjN6SDgEAAAAdJBwaBiEQwAAAEBHCYeGQTgEAAAAdJRwaBiEQwAAAEBHCYeGQTgEAAAAdJRwaBiEQwAAAEBHCYeGQTgEAAAAdJRwaBjWrhUOAQAAAJ0kHBqG8fHkkUeSvXvbrgQAAABgP8KhYRgfb84PP9xuHQAAAADzCIeGYSYcMrUMAAAA6Bjh0DAIhwAAAICOEg4Ng3AIAAAA6Cjh0DAIhwAAAICOEg4Ng3AIAAAA6Cjh0DAIhwAAAICOEg4Ng3AIAAAA6Cjh0DCsXduchUMAAABAxwiHhmHFiuTYY4VDAAAAQOcIh4ZlfFw4BAAAAHSOcGhYhEMAAABABwmHhkU4BAAAAHSQcGhYhEMAAABABwmHhkU4BAAAAHSQcGhYhEMAAABABwmHhmXt2mTHjrarAAAAANiPcGhYZkYO1dp2JQAAAAD/RDg0LOPjyd69yaOPtl0JAAAAwD8RDg3L+Hhztu4QAAAA0CHCoWERDgEAAAAdJBwaFuEQAAAA0EHCoWERDgEAAAAdJBwaFuEQAAAA0EHCoWERDgEAAAAdJBwaFuEQAAAA0EHCoWERDgEAAAAdJBwaltWrk1WrhEMAAABApwiHhml8XDgEAAAAdIpwaJiEQwAAAEDHCIeGSTgEAAAAdIxwaJiEQwAAAEDHCIeGSTgEAAAAdIxwaJiEQwAAAEDHCIeGae3aZMeOtqsAAAAA+CfCoWEycggAAADoGOHQMI2PJ7t2Jbt3t10JAAAAQBLh0HCNjzdnU8sAAACAjhAODdNMOGRqGQAAANARwqFhEg4BAAAAHSMcGibhEAAAANAxwqFhEg4BAAAAHSMcGibhEAAAANAxPYVDpZTzSyk3lFK2lVIuXeD+6lLKR6fvX1VKWT/v/hmllIdLKW/uU91Lk3AIAAAA6JhDhkOllBVJ3pPkgiQTSV5TSpmY1+31Se6vtZ6V5F1J3j7v/juT/NmRl7vECYcAAACAjull5NB5SbbVWm+qte5O8pEkF83rc1GSD04//liSl5dSSpKUUn4wyc1Jvt6XipeyY49NShEOAQAAAJ3RSzh0apLb5lzfPt22YJ9a654kDyY5qZRyXJJfSfKbR17qMlBKM3pIOAQAAAB0xKAXpP6NJO+qtT58sE6llI2llKlSytQ999wz4JJaJhwCAAAAOmRlD33uSHL6nOvTptsW6nN7KWVlkuOT3Jvk25NcXEr5/5KckGRfKWVXrfU/zP3hWuvmJJuTZHJysh7G+1g6hEMAAABAh/QSDl2d5OxSyplpQqBLkvzf8/psTfLaJH+b5OIkn6u11iQvnelQSvmNJA/PD4ZGjnAIAAAA6JBDhkO11j2llDck+UySFUmuqLV+vZRyWZKpWuvWJO9P8qFSyrYk96UJkFjI+Hhy//1tVwEAAACQpLeRQ6m1fjrJp+e1/fqcx7uS/OghnuM3DqO+5Wd8PNm+ve0qAAAAAJIMfkFq5lu71rQyAAAAoDOEQ8NmzSEAAACgQ4RDwzY+nuzYkezb13YlAAAAAMKhoRsfb84PP9xuHQAAAAARDg3fTDhkahkAAADQAcKhYRMOAQAAAB0iHBo24RAAAADQIcKhYRMOAQAAAB0iHBo24RAAAADQIcKhYRMOAQAAAB0iHBo24RAAAADQIcKhYVu7tjkLhwAAAIAOEA4N28qVyZo1wiEAAACgE4RDbRgfFw4BAAAAnSAcaoNwCAAAAOgI4VAbhEMAAABARwiH2jA+nuzY0XYVAAAAAMKhVqxda+QQAAAA0AnCoTaYVgYAAAB0hHCoDcIhAAAAoCOEQ22YCYdqbbsSAAAAYMQJh9owPp7s2ZPs2tV2JQAAAMCIEw61YXy8OZtaBgAAALRMONQG4RAAAADQEcKhNgiHAAAAgI4QDrVBOAQAAAB0hHCoDcIhAAAAoCOEQ20QDgEAAAAdIRxqg3AIAAAA6AjhUBuEQwAAAEBHCIfasHp1ctRRwiEAAACgdcKhNpTSjB4SDgEAAAAtEw61RTgEAAAAdIBwqC3j48mOHW1XAQAAAIw44VBbjBwCAAAAOkA41Ja1a4VDAAAAQOuEQ20xcggAAADoAOFQW4RDAAAAQAcIh9oiHAIAAAA6QDjUlvHx5NFHk8cfb7sSAAAAYIQJh9oyPt6cbWcPAAAAtEg41JaZcMjUMgAAAKBFwqEB2LIlWb8+GRtrzlu2LNBJOAQAAAB0wMq2C1hutmxJNm5Mdu5srm+9tblOkg0b5nQUDgEAAAAdYORQn23aNBsMzdi5s2nfj3AIAAAA6ADhUJ9t395ju3AIAAAA6ADhUJ+dcUaP7cIhAAAAoAOEQ312+eXJmjX7t61Z07TvRzgEAAAAdIBwqM82bEg2b07WrUtKac6bN89bjDpJjj226SAcAgAAAFpkt7IB2LBhgTBovrGxZO1a4RAAAADQKiOH2jQ+LhwCAAAAWiUcatP4eLJjR9tVAAAAACNMONQmI4cAAACAlgmH2mTNIQAAAKBlwqE2GTkEAAAAtEw41CbhEAAAANAy4VCbhEMAAABAy4RDbZrZrWzfvrYrAQAAAEaUcKhN4+NJrckjj7RdCQAAADCihENtGh9vzqaWAQAAAC0RDrVJOAQAAAC0TDjUJuEQAAAA0DLhUJuEQwAAAEDLhENtEg4BAAAALRMOtUk4BAAAALRMONQm4RAAAADQsp7CoVLK+aWUG0op20oply5wf3Up5aPT968qpayfbj+vlPJ308f/KqX8UJ/rX9rWrm3OwiEAAACgJYcMh0opK5K8J8kFSSaSvKaUMjGv2+uT3F9rPSvJu5K8fbr9a0kma63PT3J+kveVUlb2qfal76ijkmOOEQ4BAAAArell5NB5SbbVWm+qte5O8pEkF83rc1GSD04//liSl5dSSq11Z611z3T70UlqP4peVsbHkx072q4CAAAAGFG9hEOnJrltzvXt020L9pkOgx5MclKSlFK+vZTy9SRfTfKzc8KikbdlS7L+3msy9r73Zv365hoAAABgmAa+IHWt9apa67OTfFuSXy2lHD2/TyllYyllqpQydc899wy6pE7YsiXZuDG5dc+pqRnLrbc21wIiAAAAYJh6CYfuSHL6nOvTptsW7DO9ptDxSe6d26HWen2Sh5M8Z/4L1Fo311ona62Tp5xySu/VL2GbNiU7d+7ftnNn0w4AAAAwLL2EQ1cnObuUcmYpZVWSS5Jsnddna5LXTj++OMnnaq11+mdWJkkpZV2SZyW5pS+VL3Hbty+uHQAAAGAQDrlzWK11TynlDUk+k2RFkitqrV8vpVyWZKrWujXJ+5N8qJSyLcl9aQKkJPmuJJeWUh5Psi/Jz9da/3EQb2SpOeOM5NZbF24HAAAAGJZSa7c2EJucnKxTU1NtlzFwM2sOzZ1atmZNsnlzsmFDe3UBAAAAy08p5Zpa6+RC9wa+IDUL27ChCYLWjd+fkn1Zd0YVDAEAAABDJxxq0YYNyS2X/n72ZUVuuf5RwRAAAAAwdMKhts0sMnTzzQfttmVLsn59MjbWnG15DwAAAPSDcKhtExPN+brrDthlZn2iW29Nam3OGzcKiAAAAIAjJxxq2znnJKUcNBzatGn/hauT5nrTpgHXBgAAACx7wqG2rVmTnHnmQcOh7dsX1w4AAADQK+FQF0xMJNdff8DbM8sS9doOAAAA0CvhUBdMTCQ33JDs2bPg7csvbwYYzbVmTdMOAAAAcCSEQ10wMZHs3p3cdNOCtzdsSDZvTtata5YnWreuud6wYch1AgAAAMvOyrYLIPvvWPYt37Jglw0bhEEAAABA/xk51AXPelZzPsii1AAAAACDIBzqgrVrk9NPFw4BAAAAQycc6oqJCeEQAAAAMHTCoa6Y2c5+7962KwEAAABGiHCoKyYmkl27kltvbbsSAAAAYIQIh7pi7o5lAAAAAEMiHOqKc89tzsIhAAAAYIiEQ11x4onJ057WrDsEAAAAMCTCoS6xYxkAAAAwZMKhLpkJh2ptuxIAAABgRAiHumRiInn44eT229uuBAAAABgRwqEusWMZAAAAMGTCoS6xYxkAAAAwZMKhLjnllOTkk48oHNqyJVm/Phkba85btvStOgAAAGAZWtl2AcxzBDuWbdmSbNyY7NzZXN96a3OdJBs29Kk+AAAAYFkxcqhrjmDHsk2bZoOhGTt3Nu0AAAAACxEOdc3ERPLAA8mddy76R7dvX1w7AAAAgHCoa45gx7IzzlhcOwAAAIBwqGtmwqHrr1/0j15+ebJmzf5ta9Y07QAAAAALEQ51zVOfmpxwwmGNHNqwIdm8OVm3LimlOW/ebDFqAAAA4MDsVtY1pRzRjmUbNgiDAAAAgN4ZOdRFRxAOAQAAACyGcKiLJiaSe+5pDgAAAIABEg510REsSg0AAACwGMKhLjr33OZsahkAAAAwYMKhLjr99OS444RDAAAAwMAJh7qolGb0kHAIAAAAGDDhUFfZsQwAAAAYAuFQV01MJN/8ZvLAA21XAgAAACxjwqGusmMZAAAAMATCoa6aCYdMLQMAAAAGSDjUVevWJcccIxwCAAAABko41FUrViTPepZwCAAAABgo4VCX2bEMAAAAGDDhUJdNTCTbtyc7drRdCQAAALBMCYe67Nxzm/Pf/327dXDEtmxJ1q9Pxsaa85YtbVcEAAAADeFQl9mxbFnYsiXZuDG59dak1ua8caOACAAAgG4QDnXZM5+ZHHXUQMIhI1kOrp+/Pps2JTt37t+2c2fTDgAAAG1b2XYBHMTKlck55/Q9HJoZyTITWMyMZEmSDRv6+lJLUr9/fbZvX1w7AAAADJORQ103gB3LjGQ5uH7/+pxxxuLaAQAAYJiEQ103MZHcfHPy6KN9e8rFjGRZbtPPenk//R7pc/nlyZo1+7etWdO0AwAAQNuEQ103MdGsYnzDDX17yl5HsrS9kHK/g6le30+/R/ps2JBs3pysW5eU0pw3bz7wFLXlFsgBAADQbcKhrhvAjmW9jmRZ7PSqfoYagwimen0/ixnp0+t73rAhueWWZN++5nywYMjOZgAAAAyTcKjrzj47WbGir+FQryNZFjv9rJ+hxmKCqV4Dml7fT6+/Pm0GWAAAANAvpdbadg37mZycrFNTU22X0S3nntscn/jEUF92/fom8Jhv3bpm9Mvh9u3F2FgTuMxXSjP6Zsb8ncWSZpTPQmFOv2vs9/Mlvb9vAAAAWIxSyjW11smF7hk5tBQMYMeyXixmelW/F7nudd2fxYy06ffC0IPYot7OZgAAAAybcGgpmJhItm1LHntsqC+7mIWU+73Ida9BzmICmsUuDH0ogwhy7GwGAADAsAmHloJzz0327k3+4R+G/tK9LqTc70Wuew1yFhvQ9Pp+ejGIIKffARYAAAAcinBoKRjAjmX9NohFrnsJctocaTOoIKefARYAAAAcysq2C6AH55zTpA8dDoeSJsQ4VJBxxhkLL+J8uFOxZl5v06YmYDrjjCYYGlag0st7BgAAgC4zcmgpOOaY5BnP6Hw41ItBTcUy0gYAAAAOj3BoqZiYSK6/vu0qjpg1dQAAAKBbTCtbKiYmkj//82TPnmTl0v7PZioWAAAAdIeRQ0vFxETy+OPJjTe2XQn03ZYtyfr1ydhYc96ype2KAAAARodwaKl47nOb89RUu3XAIvQS+mzZkmzc2CxUXmtz3rhRQAQAADAswqGl4nnPS5785OS//be2K4Ge9Br6bNqU7Ny5f9vOnU07AAAAg9dTOFRKOb+UckMpZVsp5dIF7q8upXx0+v5VpZT10+3/vJRyTSnlq9Pn7+1z/aNjbCx55SuTP/uzZnoZdFyvoc/27Qv//IHaAQAA6K9DhkOllBVJ3pPkgiQTSV5TSpmY1+31Se6vtZ6V5F1J3j7d/o9JXlVrfW6S1yb5UL8KH0kXXpg8+GBy5ZVtVwKH1Gvoc8YZC/c7UDsAAAD91cvIofOSbKu13lRr3Z3kI0kumtfnoiQfnH78sSQvL6WUWuu1tdZvTLd/PckxpZTV/Sh8JH3f9yWrVydbt7ZdCRxSr6HP5Zcna9bs37ZmTdMOAADA4PUSDp2a5LY517dPty3Yp9a6J8mDSU6a1+dHkny51vrY/BcopWwspUyVUqbuueeeXmsfPcce2wREW7c2i7hAh/Ua+mzYkGzenKxbl5TSnDdvbtoBAAAYvKEsSF1KeXaaqWY/s9D9WuvmWutkrXXylFNOGUZJS9eFFyY335xcd13blcBBLSb02bAhueWWZN++5iwYAgAAGJ5ewqE7kpw+5/q06bYF+5RSViY5Psm909enJfmTJD9Ra73xSAseea98ZXM2tWzk9bJNfNuEPgAAAN3XSzh0dZKzSylnllJWJbkkyfxkYmuaBaeT5OIkn6u11lLKCUk+leTSWusX+lTzaHv605PJyeSTn2y7ElrU6zbxo2wphGcAAABdcMhwaHoNoTck+UyS65P8ca3166WUy0opF053e3+Sk0op25K8KcnMdvdvSHJWkl8vpfzd9PHkvr+LUfOqVyVf/GJy991tV0JLet0mflQJzwAAAHrX05pDtdZP11q/pdb6zFrr5dNtv15r3Tr9eFet9UdrrWfVWs+rtd403f5btdZja63Pn3NINI7UhRc2/+L91KfaroSW9LpN/HLT62igpRCeGdkEAAB0xVAWpKbPnve85PTTrTs0wnrdJn45WcxooK6HZ22PbBJMAQAAcwmHlqJSmqllf/EXya5dbVdDC3rdJn45WcxooEGEZ/0MVAY1sqmXGhcTTC23EGm5vR8AAOgX4dBSdeGFzb8mP/e5tiuhBYvZJn4Q2vhH9mJGAy0mPGsjUBnEyKZea+w1mFrs6KZef0/0+/fOYl7XOlQAAHAAtdZOHS960YsqPdi1q9bjjqv1Z36m7UoYMX/0R7WuWVNr80/s5lizpmkfpHXr9n/NmWPdugPXuW5draU054Xq6/W99Pra/X6+xbyXXp+zlIX7lXL4Nfb6vvv9e2cxz7fYX3MAAFhukkzVA2QxpbnfHZOTk3VqaqrtMpaGiy9O/vZvk9tvb4aPwBHYsqUZPbJ9ezP96vLLFx6JtH59M+pivnXrkltuGWx9GzfuP+plzZojGzHV63sZG2uihPlKSfbtW/zzLea99Nq33zX2+nyLec7F/N7p5ffjYp5vMe+n188CAAAsJaWUa2qtkwvdM61sKXvVq5JvfCP58pfbroQO6/e0qbYWex7EVLpe30uvaxj1+nyLeS+9TgPrtcZep9wtZt2mXt93r/16/f24mN+Lvb4f08/omn5P2VzM1M62XluNauzSa6tRjV16bTV2t8Zl4UBDito6TCtbhLvvrnVsrNZ//a/broSOGsQ0p+U0Paft6WK96HUa2GKmWPVzyt1i3ndb/RbzfpbT7+/lppfft4vt21a/Xvv2e8rmYv+caOO11ahGNapRjWrsQr/F9l0qcpBpZQs2tnkIhxbpJS+p9QUvaLsKOqrf69DUurz+kGwzUOnVYgOQXv9h2ovF/IO4n1/GgwjEen0/i/0s9PPXezG6Hmr0u9+o/mWzzUB1KYS+alSjGtWoRjV2ocalRDi0nL397c1/xttua7sSOmgQiw/X2u4/ivutrUBlMc+3FMK4fgYBi/n92O9f715fu9/B4mL6LoVQo9/9RvUvm73+Gd7vfm2+thrVqEY1qlGNXei32L5LhXBoObvuuuY/43vf23YldNAg/qHL8C2nMK4Xbf5+7HdYYTRJf/qN6l82R/G/tRrVqEY1qlGNXei32L5LhXBoOdu3r9azzqr1ggvaroQOGtToBhi0rk/ZGsSoPKNJhvvruBT+sjmKo8TUqEY1qlGNauxCv8X2XSqEQ8vdm95U66pVte7Y0XYldJDQB/qvzREvSyHU6He/Uf7LZq9/hve7X5uvrUY1dum11ajGLr22Grtb41IhHFru/vqvm/+UH/9425UAjIRe/3FvNEl/X9tfNgEADp9waLnbvbvWE0+s9XWva7sSgJHRyz/ujSbp72sDAHD4DhYOleZ+d0xOTtapqam2y1h6NmxI/uIvkjvvTFasaLsaAKZt2ZJs2pRs356ccUZy+eXNH9lH2hcAABajlHJNrXVywXvCoWXiox9NLrkk+cIXkhe/uO1qAAAAgA45WDg0NuxiGJDzz09Wrky2bm27EgAAAGAJEQ4tF8cfn3z3dyef/GTblQAAAABLiHBoObnwwuS665Jt29quBAAAAFgihEPLyate1ZyNHgIAAAB6JBxaTs48M3nOc4RDAAAAQM+EQ8vNq16VfP7zyf33t10JAAAAsAQIh5abCy9M9u5N/uzP2q4EAAAAWAKEQ8vNeeclZ5yRvPvdyb59bVcDAAAAdJxwaLkZG0t+67eSq69OPvzhtqsBAAAAOk44tBxt2JBMTia/+qvJzp1tVwMAAAB0mHBoORobS975zuT225Pf+Z22qwEAAAA6TDi0XL30pcnFFydve1vyjW+0XQ0AAADQUcKh5extb0v27El+7dfargQAAADoKOHQcvbMZya/+IvJBz6QXHtt29UAAAAAHSQcWu42bUpOOil505uSWtuuBgAAAOgY4dByd8IJyW/+ZvI3f5Ns3dp2NQAAAEDHCIdGwcaNybnnJm9+c7J7d9vVAAAAAB0iHBoFK1c2W9pv25a8971tVwMAAAB0iHBoVFxwQfIDP9BMMbv33rarAQAAADpCODRK3vGO5KGHkssua7sSAAAAoCOEQ6PkOc9Jfvqnm6llN9zQdjUAAABABwiHRs1llyXHHJO85S1tVwIAAAB0gHBo1Dz5ycmmTcknP5n81V+1XQ0AAADQMuHQKHrjG5P165M3vSnZu7ftagAAAIAWCYdG0dFHJ29/e/KVryR/8AdtVwMAAAC0SDg0qn70R5OXvCT5lV9JvvGNtqsBAAAAWiIcGlWlJO9/f/Loo8nrXpfs29d2RQAAAEALhEOj7Jxzkne+M/nsZ5N//+/brgYAAABogXBo1P3MzySvfGUzvexrX2u7GgAAAGDIhEOjbmZ62fHHJxs2JI891nZFAAAAwBAJh0ie/OTkiiua3cs2bWq7GgAAAGCIhEM0XvGK5Od+Lvmd30n+6q/argYAAAAYEuEQs97xjmaR6te+NrnvvrarAQAAAIZAOMSsNWuSLVuSu+5KfvZnk1rbrggAAAAYMOEQ+3vRi5J/82+S//pfkw99qO1qAAAAgAETDvFEb3lL8rKXJW94Q3LzzW1XAwAAAAyQcIgnWrEi+cM/bLa5//EfT/bsabsiAAAAYECEQyxs3brkve9NvvCF5G1va7saAAAAYECEQxzYhg3Ja16T/MZvJF/6UtvVAAAAAAMgHOLg3vve5OlPT37sx5KHHmq7GgAAAKDPhEMc3AknJH/0R8lNNyWvf73t7QEAAGCZEQ5xaC97WfLbv5187GPJu9/ddjUAAABAHwmH6M2b35z80A8129xfeWXb1QAAAAB9IhyiN6Ukf/AHyTOekbz61cmdd7ZdEQAAANAHwiF6d/zxycc/njzwQHLJJcmePW1XBAAAABwh4RCL89znJps3J//9vydvfWvb1QAAAABHSDjE4v3YjyU/93PJv/t3ySc+0XY1AAAAwBEQDnF43vWu5Nu+LXnd65L//b/brgYAAAA4TMIhDs/q1c3W9qtWJT/yI8kjj7RdEQAAAHAYegqHSinnl1JuKKVsK6VcusD91aWUj07fv6qUsn66/aRSyl+XUh4upfyHPtdO2844I/nwh5Ovfz352Z9Nam27IgAAAGCRDhkOlVJWJHlPkguSTCR5TSllYl631ye5v9Z6VpJ3JXn7dPuuJP8qyZv7VjHd8v3fn/zmbyZ/9EfJ+97XdjUAAADAIvUycui8JNtqrTfVWncn+UiSi+b1uSjJB6cffyzJy0sppdb6SK31f6QJiViuNm1KLrggeeMbky99qe1qAAAAgEXoJRw6Ncltc65vn25bsE+tdU+SB5Oc1I8CWQLGxpqRQ097WnLxxck997RdEQAAANCjTixIXUrZWEqZKqVM3SNYWJqe9KTk4x9vgqELL0wefbTtigAAAIAe9BIO3ZHk9DnXp023LdinlLIyyfFJ7u21iFrr5lrrZK118pRTTun1x+iaF72oGUF01VXJj/94sm9f2xUBAAAAh9BLOHR1krNLKWeWUlYluSTJ1nl9tiZ57fTji5N8rlZbV42kH/mR5Hd+pxlF9Ja3tF0NAAAAcAgrD9Wh1rqnlPKGJJ9JsiLJFbXWr5dSLksyVWvdmuT9ST5UStmW5L40AVKSpJRyS5LxJKtKKT+Y5Ptrrdf1/Z3QHb/0S8nNNyfvfGdy5pnJG97QdkUAAADAAZSuDfCZnJysU1NTbZfBkdq7txlF9MlPJn/yJ806RAAAAEArSinX1FonF7rXiQWpWYZWrEg+/OFmHaJLLkmuvrrtigAAAIAFCIcYnDVrmpFDT31q8spXNlPNAAAAgE4RDjFYT3lK8ulPJ48/nlxwQXLffW1XBAAAAMwhHGLwnvWs5E//tBk59EM/lDz2WNsVAQAAANOEQwzHy16WfOADyec/n/zkTyb79rVdEQAAAJAetrKHvnnNa5Jbbkne+tZk/frk3/7btisCAACAkSccYrguvbSZXvbbv5086UnJL/9yUkrbVQEAAMDIEg4xXKUk731vszD1W96S3Hhj8nu/lxx1VNuVAQAAwEiy5hDDt3Jl8sd/nPzKryS///vJK16RPPBA21UBAADASBIO0Y6xseRtb0uuuCL5m79JvvM7m1FEAAAAwFAJh2jXT/5k8tnPJnffnXz7tydXXtl2RQAAADBShEO077u/O/niF5OTTkpe/vLkD/+w7YoAAABgZAiH6Iazz24Cope+NHnta5Nf+7Vk3762qwIAAIBlTzhEd5x4YvLnf5781E8ll1+evPrVyc6dbVcFAAAAy5pwiG456qhk8+bkHe9IPv7xZsrZN7/ZdlUAAACwbAmH6J5Skl/+5eRP/zS5/vrknHOa69tua7syAAAAWHaEQ3TXhRcmV1+dvOpVye/+bvKMZyQ/8RPJV77SdmUAAACwbAiH6LZzz022bEluvDH5hV9IPvGJ5HnPS84/P/nc55Ja264QAAAAljThEEvDunXJu9+dbN/eLFZ97bXNtveTk8lHPpLs2dN2hQAAALAkCYdYWp70pOStb01uvbVZuPrhh5PXvCY5++zk934veeihtisEAACAJUU4xNJ09NHJT/90s2D1n/xJ8vSnJ298Y3Lqqcm//JfJDTe0XSEAAAAsCcIhlraxseQHfzD5wheSq65qHr/vfcmzntWsS/SpTyX79rVdJQAAAHSWcIjl47zzkg99qNny/rLLml3NXvnK5Fu+JXnXu5IHHmi7QgAAAOgc4RDLz1Oekvyrf9WsS/SRjyRPfWrypjclp52W/PzPJ9dd13aFAAAA0BnCIZavo45KXv3q5H/8j+Saa5J/8S+SK65Inv3s5JxzmqDoE59I7r+/7UoBAACgNaXW2nYN+5mcnKxTU1Ntl8Fy9Y//mHz4w8lnP5v8zd80u52NjSUvelHyfd/XHC9+cbPgNQAAACwTpZRraq2TC94TDjGyHn88+dKXkr/8y+b44heTPXuaYOi7vqsJir7ne5IXvrAZhQQAAABLlHAIerFjR/L5z8+GRV/7WtO+Zk3ynd+ZvPSlzfEd39G0AQAAwBIhHILDceedyZVXzh7/638ltSYrVzbT0F760uRlL0te8pLkSU9qu1oAAAA4IOEQ9MODDyb/8382o4uuvDK5+upk9+7m3rnnJt/2bc0xOZk8//nWLQIAAKAzhEMwCLt2NWsWXXllctVVTVh0553NvZUrk+c8Z//A6DnPsXYRAAAArRAOwTDUmtxxRzI11QRFM+f772/ur16dfOu3Js96VnLOObPH2WcbZQQAAMBACYegLbUmN900GxT93d8lN9yQ3H77bJ9SknXrmqBoJjiamEhe/GIjjQAAAOiLg4VDK4ddDIyUUpJnPrM5Xv3q2faHH07+4R+Sv//7JiyaOa68Mtm5s+nz1Kcmr3td8vrXJ2ed1Ur5AAAALH9GDkGX7NvXTE27+urkAx9IPvWppu2f/bPkp34q+eEfNgUNAACARTvYyKGxYRcDHMTYWHL66U0ItHVrsn178lu/ldxyS7JhQ/L0pye/+IvJV77SdqUAAAAsE8Ih6LJTT002bUq2bUv+8i+TH/iB5H3vS573vOTbvz35T/8pufnmZnQRAAAAHAZrDsFSMDaWvPzlzXHvvcmHPtQEQxs3NvePOaZZyPrcc/c/zj47WbWq3doBAADoNGsOwVJVa/LlLzfH9dfPHrfeOttnxYpmMexzz02+9Vub0UbnnZecckp7dQMAADB0diuD5aiU5EUvao65Hnmk2flsbmB03XXJJz85O/3szDObkGgmLHrhC5vRRwAAAIwc4RAsN8ce24Q9L3zh/u2PPJJcc03ypS8lV12V/O3fJh/9aHNvxYrZkUWTk83jZz87WbNm+PUDAAAwVKaVwSj75jeTq69uwqIvfak5HnqouVdKctZZyXOfu//xzGc2YRIAAABLxsGmlQmHgFn79iU33ph89avN8ZWvNOdt25o1jpJm+tnERPKc5yTr1yenn77/cdxxrb4FAAAAnsiaQ0BvxsaaHc7OPjv54R+ebd+5s1m3aCYs+upXk898Jrnzzic+xwknNCHRaafNBkannZY8/enJqac25xNOaEYmAQAA0DrhEHBoa9Y0axFNzguZd+9O7rgjue225Pbbm/PMcfvtydRUcs89T3y+Y45pQqK5gdGppyZPe1ry1KcmT3lKcz7xRCESAADAgAmHgMO3alWz89mZZx64z65dzdpGd9zRHN/4xv7nqanm/OijT/zZo46aDYrmhkZPfnJy/PHNCKT55/FxayIBAAAsgnAIGKyjjz50gFRr8sADzTS1O+9M7rrriY9vv73Zbe2uu5q1kQ5m7domLJofHC0UJs2czz47OemkPr1pAACApUM4BLSvlGYK2YknJueee/C+e/cm99+fPPhgczzwwBMfzz/fdVdyww2zbXv2LPzcp5+evPCFyQte0BwvfGEz3c3UNgAAYBkTDgFLy4oVycknN8fhqLWZwjY3QLrvvuT665Mvfzm59tpk69bZ3dlOPnk2KHrBC5JnPKPZke3YY5vzccc10+sAAACWKOEQMFpKaRbYXrOmWQh7xiteMfv44YebndmuvbY5vvzl5J3vTB5/fOHnXLlyNiiaCY3Wrn3iVLaFjuOPb/oee2wzBc8oJQAAYMiEQwDzHXdc8uIXN8eM3buT665r1j56+OHkkUea8/zHM9cPPZTceGMzMumBB5IdOw79umNjTxyVNP967drZ84Eer1o1O/Jp7nl+26pVs4t7C6UAAGBkCYcAerFqVfL85zfH4dizpwmMZsKimSltDzywcMg09/G99ya33DJ7vWNHs/ZSv6xe3ewEN3PM7Aw3d3e4E0+cXeR77Vo7wgEAwDIiHAIYhpUrkyc9qTmOVK3JY481IdGOHbOB0cx5ZvrbzGiguee5j3ftSu6+e3ZHuLvuakZGTU017QfbFW6hHeGOP352pNPMaKeZxwc61qxpzscc04ycAgAAhk44BLDUlNKsT3T00ckppwzmNfbubUYs3XVXExTN3wFu/nHnnc2OcDOjnh55ZHb6Wq+OOWb/wGhmbahjjpl9v3Mfz71evbp5vX37nnjs3bv/9Zo1s2HWQsfatU2YBwAAI8LffgF4ohUrmulkT37y4f18rc3IpJmgaG5o9Mgjyc6dBz7Pb3vooSag2rWr2Wlu167Zx7t3917TzMipg42ImnHsscn4+Ox6TvOP+Ws9zYRYM+cDPV692vpOAAB0jnAIgP4rZTYYOfnkwb3Ovn1NUPTYY820tIWOFSv2n1K3a1cz2umhhxYeBTVzzEzbmzluu23/6Xs7dy6+3rGx2RFRc0dHzX88d3TUzLF69cJtq1YtfCx076ijTN8DAOAJhEMALF1zw5ZezQQrT3nKkb323r2zi4Q/+mgTFs09z2+beTx3hNTcUVJ33z17PTM66rHHmnM/HXXUgQOluSHS/McLnQ/2eKHrXo6VK2fPcx/PnGfCPgAA+kY4BACHY8WK2XWKBqnWZvrc/MBoZlrd7t1N28zjXq7nHzP3H3usWdD88ceb64ceas4z1zPnmcdz2xe7xtSRGBubDYpmjgNd93o+2OOFrg/VfjjHzEi3g13341jMcwniAGAkCIcAoMtKaaaIrV49+CDqSOzdu39YNP/xoY7du2efY8+e2fPcxzPnvXtnz3OPuW0LPV7o/Oiji3uOg73mcrXY8KmUw+t3oOuDnQ/V1s/7/eh7sP4LXc/fbbKfx0KvdbjH4da40M/NbVvs48Xc69fz9qvGueeF2no9Axwm4RAAcORmRrccfXTblbRnZne8wz3m/vz851poJ74D7c7Xy859B+vX6/MdrN/BfmZu3/n9DnRd62wYN7995nygxwv162ffgz0e5og6mOtA4dPhhk79DLAO9ly9vtaB2g72vEf63MOucTFth/v8w3yOYbYdyc8f6P1cemnyPd+z8GstE8IhAIB+mBnxctRRbVdCl/QaNh0oWJp73e9j/usdznG4NS70c3PbFvt4Mff69bz9qnHueaG2wzn32nY4537VOP+5en2tA7Ud7HmP9LmHXeNi2g73+Yf5HIfTdrjPdySvfbD3s5gdcpco4RAAAAxKKc2oOgDoMPvZAgAAAIww4RAAAADACBMOAQAAAIww4RAAAADACOspHCqlnF9KuaGUsq2UcukC91eXUj46ff+qUsr6Ofd+dbr9hlLKD/SxdgAAAACO0CHDoVLKiiTvSXJBkokkrymlTMzr9vok99daz0ryriRvn/7ZiSSXJHl2kvOTvHf6+QAAAADogF5GDp2XZFut9aZa6+4kH0ly0bw+FyX54PTjjyV5eSmlTLd/pNb6WK315iTbpp8PAAAAgA7oJRw6Ncltc65vn25bsE+tdU+SB5Oc1OPPAgAAANCSTixIXUrZWEqZKqVM3XPPPW2XAwAAADAyegmH7khy+pzr06bbFuxTSlmZ5Pgk9/b4s6m1bq61TtZaJ0855ZTeqwcAAADgiPQSDl2d5OxSypmllFVpFpjeOq/P1iSvnX58cZLP1VrrdPsl07uZnZnk7CRf6k/pAAAAAByplYfqUGvdU0p5Q5LPJFmR5Ipa69dLKZclmaq1bk3y/iQfKqVsS3JfmgAp0/3+OMl1SfYk+YVa694BvRcAAAAAFqk0A3y6Y3Jysk5NTbVdBgAAAMCyUUq5ptY6udC9TixIDQAAAEA7hEMAAAAAI0w4BAAAADDChEMAAAAAI0w4BAAAADDCOrdbWSnlniS3tl1Hn5yc5B/bLgKWEJ8ZWByfGVgcnxlYHJ8ZWJyuf2bW1VpPWehG58Kh5aSUMnWgbeKAJ/KZgcXxmYHF8ZmBxfGZgcVZyp8Z08oAAAAARphwCAAAAGCECYcGa3PbBcAS4zMDi+MzA4vjMwOL4zMDi7NkPzPWHAIAAAAYYUYOAQAAAIww4dCAlFLOL6XcUErZVkq5tO16oEtKKaeXUv66lHJdKeXrpZQ3Trc/qZTy2VLKP0yfT2y7VuiSUsqKUsq1pZT/Nn19Zinlqunvmo+WUla1XSN0RSnlhFLKx0opf19Kub6U8p2+Z+DASin/z/Tfy75WSvkvpZSjfc/A/kopV5RS7i6lfG1O24LfLaXxe9Ofn6+UUl7YXuWHJhwagFLKiiTvSXJBkokkrymlTLRbFXTKniS/XGudSPIdSX5h+jNyaZK/qrWeneSvpq+BWW9Mcv2c67cneVet9awk9yd5fStVQTf9bpI/r7U+K8nz0nx2fM/AAkoppyb5xSSTtdbnJFmR5JL4noH5PpDk/HltB/puuSDJ2dPHxiT/cUg1Hhbh0GCcl2RbrfWmWuvuJB9JclHLNUFn1Fq/WWv98vTjHWn+wn5qms/JB6e7fTDJD7ZSIHRQKeW0JK9I8p+nr0uS703ysekuPjMwrZRyfJKXJXl/ktRad9daH4jvGTiYlUmOKaWsTLImyTfjewb2U2v9fJL75jUf6LvloiR/WBtfTHJCKeVpQyn0MAiHBuPUJLfNub59ug2Yp5SyPskLklyV5Cm11m9O37ozyVPaqgs66N1J/t8k+6avT0ryQK11z/S17xqYdWaSe5L8wfRUzP9cSjk2vmdgQbXWO5K8I8n2NKHQg0muie8Z6MWBvluWVC4gHAJaU0o5LsnHk/xSrfWhufdqs5Wi7RQhSSnllUnurrVe03YtsESsTPLCJP+x1vqCJI9k3hQy3zMwa3qNlIvSBKtPT3Jsnjh1BjiEpfzdIhwajDuSnD7n+rTpNmBaKeWoNMHQllrrJ6ab75oZajl9vrut+qBjXpLkwlLKLWmmKn9vmvVUTpge/p/4roG5bk9ye631qunrj6UJi3zPwMK+L8nNtdZ7aq2PJ/lEmu8e3zNwaAf6bllSuYBwaDCuTnL29Or+q9Is5ra15ZqgM6bXSnl/kutrre+cc2trktdOP35tkv9/2LVBF9Vaf7XWelqtdX2a75TP1Vo3JPnrJBdPd/OZgWm11juT3FZKOWe66eVJrovvGTiQ7Um+o5SyZvrvaTOfGd8zcGgH+m7ZmuQnpnct+44kD86ZftY5pRn1RL+VUv6vNOtDrEhyRa318nYrgu4opXxXkiuTfDWz66e8Nc26Q3+c5Iwktyb5F7XW+Qu+wUgrpXxPkjfXWl9ZSnlGmpFET0pybZIfq7U+1mJ50BmllOenWcB9VZKbkvxkmv8x6nsGFlBK+c0kr06zq+y1SX4qzfoovmdgWinlvyT5niQnJ7kryb9O8qdZ4LtlOmj9D2mmaO5M8pO11qkWyu6JcAgAAABghJlWBgAAADDChEMAAAAAI0w4BAAAADDChEMAAAAAI0w4BAAAADDChEMAAAAAI0w4BAAAADDChEMAAAAAI+z/AEhO2CoQESmJAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2 = UrbanGreenRegression()\n",
    "criterion2 = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer2 = torch.optim.SGD(model2.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer2, 'min', patience=5, factor=0.75)\n",
    "best_model_path = train_model(model2, Dataloaders_NON_OHE, criterion2, optimizer2, scheduler2, device, num_epochs=100, batch_size=batch_size, path='/home/bcyoon/Byeongchan/Data/N12/Model/Segmentation/Regression/', description='patch_50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBN(nn.Module):\n",
    "    def __init__(self, Cin, Cout, kernel_size, stride=1):\n",
    "        super(ConvBN,self).__init__()\n",
    "        self.conv = nn.Conv2d(Cin, Cin, kernel_size=kernel_size, stride=stride, groups=Cin, bias=False)\n",
    "        self.batchnorm = nn.BatchNorm2d(Cin)\n",
    "        self.pointwise = nn.Conv2d(Cin, Cout, kernel_size=1, padding=0, bias=False)\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [16, 64, 138, 138]             576\n",
      "       BatchNorm2d-2         [16, 64, 138, 138]             128\n",
      "            Conv2d-3        [16, 128, 138, 138]           8,192\n",
      "            ConvBN-4        [16, 128, 138, 138]               0\n",
      "              ReLU-5        [16, 128, 138, 138]               0\n",
      "            Conv2d-6        [16, 128, 136, 136]           1,152\n",
      "       BatchNorm2d-7        [16, 128, 136, 136]             256\n",
      "            Conv2d-8        [16, 128, 136, 136]          16,384\n",
      "            ConvBN-9        [16, 128, 136, 136]               0\n",
      "             ReLU-10        [16, 128, 136, 136]               0\n",
      "================================================================\n",
      "Total params: 26,688\n",
      "Trainable params: 26,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 76.56\n",
      "Forward/backward pass size (MB): 2635.25\n",
      "Params size (MB): 0.10\n",
      "Estimated Total Size (MB): 2711.91\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_for_summary = SegBlock(skip=True, in_channel=64, out_channel=128)\n",
    "model_for_summary.to(device)\n",
    "summary(model_for_summary, input_size=(64,140,140), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_add(skip:Type[torch.Tensor], target:Type[torch.Tensor])->torch.Tensor:\n",
    "    cropped_skip = skip[:,:,(skip.shape[-2]-target.shape[-2])//2:(skip.shape[-2]+target.shape[-2])//2,(skip.shape[-1]-target.shape[-1])//2:(skip.shape[-1]+target.shape[-1])//2]\n",
    "    return torch.cat((cropped_skip, target), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net 마지막에 Fully Convolutional Layer 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegBlock(nn.Module):\n",
    "    def __init__(self, skip:bool, in_channel:int, out_channel:int, is_last:bool = False):\n",
    "        super(SegBlock,self).__init__()\n",
    "        self.skip = skip\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = ConvBN(in_channel, out_channel, kernel_size=3, stride=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = ConvBN(out_channel, out_channel, kernel_size=3, stride=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.enc1 = SegBlock(True, 3,64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = SegBlock(True, 64,128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = SegBlock(True, 128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.enc4 = SegBlock(True, 256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        self.dec0 = SegBlock(False, 512, 1024)\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.dec1 = SegBlock(False,1024,512)\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec2 = SegBlock(False,512,256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec3 = SegBlock(False,256,128)\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec4 = SegBlock(False,128,64, is_last = True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        enc1 = self.enc1(x)\n",
    "        x = self.pool1(enc1)\n",
    "        enc2 = self.enc2(x)\n",
    "        x = self.pool2(enc2)\n",
    "        enc3 = self.enc3(x)\n",
    "        x = self.pool3(enc3)\n",
    "        enc4 = self.enc4(x)\n",
    "        x = self.pool4(enc4)\n",
    "        x = self.dec0(x)\n",
    "        x = self.upconv1(x)\n",
    "        x = self.dec1(crop_add(enc4, x))\n",
    "        x = self.upconv2(x)\n",
    "        x = self.dec2(crop_add(enc3, x))\n",
    "        x = self.upconv3(x)\n",
    "        x = self.dec3(crop_add(enc2, x))\n",
    "        x = self.upconv4(x)\n",
    "        x = self.dec4(crop_add(enc1, x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [8, 3, 282, 282]              27\n",
      "       BatchNorm2d-2           [8, 3, 282, 282]               6\n",
      "            Conv2d-3          [8, 64, 282, 282]             192\n",
      "            ConvBN-4          [8, 64, 282, 282]               0\n",
      "              ReLU-5          [8, 64, 282, 282]               0\n",
      "            Conv2d-6          [8, 64, 280, 280]             576\n",
      "       BatchNorm2d-7          [8, 64, 280, 280]             128\n",
      "            Conv2d-8          [8, 64, 280, 280]           4,096\n",
      "            ConvBN-9          [8, 64, 280, 280]               0\n",
      "             ReLU-10          [8, 64, 280, 280]               0\n",
      "         SegBlock-11          [8, 64, 280, 280]               0\n",
      "        MaxPool2d-12          [8, 64, 140, 140]               0\n",
      "           Conv2d-13          [8, 64, 138, 138]             576\n",
      "      BatchNorm2d-14          [8, 64, 138, 138]             128\n",
      "           Conv2d-15         [8, 128, 138, 138]           8,192\n",
      "           ConvBN-16         [8, 128, 138, 138]               0\n",
      "             ReLU-17         [8, 128, 138, 138]               0\n",
      "           Conv2d-18         [8, 128, 136, 136]           1,152\n",
      "      BatchNorm2d-19         [8, 128, 136, 136]             256\n",
      "           Conv2d-20         [8, 128, 136, 136]          16,384\n",
      "           ConvBN-21         [8, 128, 136, 136]               0\n",
      "             ReLU-22         [8, 128, 136, 136]               0\n",
      "         SegBlock-23         [8, 128, 136, 136]               0\n",
      "        MaxPool2d-24           [8, 128, 68, 68]               0\n",
      "           Conv2d-25           [8, 128, 66, 66]           1,152\n",
      "      BatchNorm2d-26           [8, 128, 66, 66]             256\n",
      "           Conv2d-27           [8, 256, 66, 66]          32,768\n",
      "           ConvBN-28           [8, 256, 66, 66]               0\n",
      "             ReLU-29           [8, 256, 66, 66]               0\n",
      "           Conv2d-30           [8, 256, 64, 64]           2,304\n",
      "      BatchNorm2d-31           [8, 256, 64, 64]             512\n",
      "           Conv2d-32           [8, 256, 64, 64]          65,536\n",
      "           ConvBN-33           [8, 256, 64, 64]               0\n",
      "             ReLU-34           [8, 256, 64, 64]               0\n",
      "         SegBlock-35           [8, 256, 64, 64]               0\n",
      "        MaxPool2d-36           [8, 256, 32, 32]               0\n",
      "           Conv2d-37           [8, 256, 30, 30]           2,304\n",
      "      BatchNorm2d-38           [8, 256, 30, 30]             512\n",
      "           Conv2d-39           [8, 512, 30, 30]         131,072\n",
      "           ConvBN-40           [8, 512, 30, 30]               0\n",
      "             ReLU-41           [8, 512, 30, 30]               0\n",
      "           Conv2d-42           [8, 512, 28, 28]           4,608\n",
      "      BatchNorm2d-43           [8, 512, 28, 28]           1,024\n",
      "           Conv2d-44           [8, 512, 28, 28]         262,144\n",
      "           ConvBN-45           [8, 512, 28, 28]               0\n",
      "             ReLU-46           [8, 512, 28, 28]               0\n",
      "         SegBlock-47           [8, 512, 28, 28]               0\n",
      "        MaxPool2d-48           [8, 512, 14, 14]               0\n",
      "           Conv2d-49           [8, 512, 12, 12]           4,608\n",
      "      BatchNorm2d-50           [8, 512, 12, 12]           1,024\n",
      "           Conv2d-51          [8, 1024, 12, 12]         524,288\n",
      "           ConvBN-52          [8, 1024, 12, 12]               0\n",
      "             ReLU-53          [8, 1024, 12, 12]               0\n",
      "           Conv2d-54          [8, 1024, 10, 10]           9,216\n",
      "      BatchNorm2d-55          [8, 1024, 10, 10]           2,048\n",
      "           Conv2d-56          [8, 1024, 10, 10]       1,048,576\n",
      "           ConvBN-57          [8, 1024, 10, 10]               0\n",
      "             ReLU-58          [8, 1024, 10, 10]               0\n",
      "         SegBlock-59          [8, 1024, 10, 10]               0\n",
      "  ConvTranspose2d-60           [8, 512, 20, 20]       2,097,664\n",
      "           Conv2d-61          [8, 1024, 18, 18]           9,216\n",
      "      BatchNorm2d-62          [8, 1024, 18, 18]           2,048\n",
      "           Conv2d-63           [8, 512, 18, 18]         524,288\n",
      "           ConvBN-64           [8, 512, 18, 18]               0\n",
      "             ReLU-65           [8, 512, 18, 18]               0\n",
      "           Conv2d-66           [8, 512, 16, 16]           4,608\n",
      "      BatchNorm2d-67           [8, 512, 16, 16]           1,024\n",
      "           Conv2d-68           [8, 512, 16, 16]         262,144\n",
      "           ConvBN-69           [8, 512, 16, 16]               0\n",
      "             ReLU-70           [8, 512, 16, 16]               0\n",
      "         SegBlock-71           [8, 512, 16, 16]               0\n",
      "  ConvTranspose2d-72           [8, 256, 32, 32]         524,544\n",
      "           Conv2d-73           [8, 512, 30, 30]           4,608\n",
      "      BatchNorm2d-74           [8, 512, 30, 30]           1,024\n",
      "           Conv2d-75           [8, 256, 30, 30]         131,072\n",
      "           ConvBN-76           [8, 256, 30, 30]               0\n",
      "             ReLU-77           [8, 256, 30, 30]               0\n",
      "           Conv2d-78           [8, 256, 28, 28]           2,304\n",
      "      BatchNorm2d-79           [8, 256, 28, 28]             512\n",
      "           Conv2d-80           [8, 256, 28, 28]          65,536\n",
      "           ConvBN-81           [8, 256, 28, 28]               0\n",
      "             ReLU-82           [8, 256, 28, 28]               0\n",
      "         SegBlock-83           [8, 256, 28, 28]               0\n",
      "  ConvTranspose2d-84           [8, 128, 56, 56]         131,200\n",
      "           Conv2d-85           [8, 256, 54, 54]           2,304\n",
      "      BatchNorm2d-86           [8, 256, 54, 54]             512\n",
      "           Conv2d-87           [8, 128, 54, 54]          32,768\n",
      "           ConvBN-88           [8, 128, 54, 54]               0\n",
      "             ReLU-89           [8, 128, 54, 54]               0\n",
      "           Conv2d-90           [8, 128, 52, 52]           1,152\n",
      "      BatchNorm2d-91           [8, 128, 52, 52]             256\n",
      "           Conv2d-92           [8, 128, 52, 52]          16,384\n",
      "           ConvBN-93           [8, 128, 52, 52]               0\n",
      "             ReLU-94           [8, 128, 52, 52]               0\n",
      "         SegBlock-95           [8, 128, 52, 52]               0\n",
      "  ConvTranspose2d-96          [8, 64, 104, 104]          32,832\n",
      "           Conv2d-97         [8, 128, 102, 102]           1,152\n",
      "      BatchNorm2d-98         [8, 128, 102, 102]             256\n",
      "           Conv2d-99          [8, 64, 102, 102]           8,192\n",
      "          ConvBN-100          [8, 64, 102, 102]               0\n",
      "            ReLU-101          [8, 64, 102, 102]               0\n",
      "          Conv2d-102          [8, 64, 100, 100]             576\n",
      "     BatchNorm2d-103          [8, 64, 100, 100]             128\n",
      "          Conv2d-104          [8, 64, 100, 100]           4,096\n",
      "          ConvBN-105          [8, 64, 100, 100]               0\n",
      "            ReLU-106          [8, 64, 100, 100]               0\n",
      "        SegBlock-107          [8, 64, 100, 100]               0\n",
      "================================================================\n",
      "Total params: 5,988,065\n",
      "Trainable params: 5,988,065\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 7.38\n",
      "Forward/backward pass size (MB): 6575.87\n",
      "Params size (MB): 22.84\n",
      "Estimated Total Size (MB): 6606.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_for_summary = UNet()\n",
    "model_for_summary.to(device)\n",
    "summary(model_for_summary, input_size=(3,284,284), batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanGreenSegmentation(pl.LightningModule):\n",
    "    def __init__(self, rotate_training_data : bool = False, train_ratio : float = 0.8, patch_size : int = 100, batch_size : int = 16):\n",
    "        super(UrbanGreenSegmentation, self).__init__()\n",
    "        raw_data_array, OHE_target_array, raw_target_array = prepare_raw_files()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.Datasets = {\n",
    "            'Train' : TrainDataset2(raw_data_array, OHE_target_array, raw_target_array, patch_size = patch_size, rotate = rotate_training_data, train_ratio = train_ratio),\n",
    "            'Validation' : TrainDataset2(raw_data_array, OHE_target_array, raw_target_array, patch_size = patch_size, is_validating = True, rotate = rotate_training_data, train_ratio = train_ratio),\n",
    "            'Prediction' : TrainDataset2(raw_data_array, OHE_target_array, raw_target_array, patch_size = patch_size, is_evaluating = True, train_ratio = train_ratio)\n",
    "        }\n",
    "        \n",
    "        self.Dataloaders = {\n",
    "            'Train' : DataLoader(self.Datasets['Train'], batch_size=batch_size),\n",
    "            'Validation' : DataLoader(self.Datasets['Validation'], batch_size=batch_size),\n",
    "            'Prediction' : DataLoader(self.Datasets['Prediction'], batch_size=batch_size)\n",
    "        }\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        train_optimizer = torch.optim.Adam(self.parameters(), lr=0.02)\n",
    "        train_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(train_optimizer, T_max=10)\n",
    "        return [train_optimizer], [train_scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.Datasets['Train'], batch_size = self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.Datasets['Validation'], batch_size = self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.Datasets['Prediction'], batch_size = self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8bef1a0741f78125b97ca6015f4b21165d553afbb2c419d3dfb1350931d81372"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
